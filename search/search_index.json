{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Cost Handbook # The Cloud Cost Handbook is a free, open-source, community-supported set of guides meant to help explain often-times complex pricing of public cloud infrastructure and service providers in easy-to-understand terms. This guide is hosted on Github and is open to anyone to contribute their knowledge to the community. Vantage employees will maintain hosting the guide for everyone and ensure that content is relevant and adheres to styleguides. Structure # This handbook is separated into two different sections for the time-being as explained below: General Concepts # These are general concepts that don't necessarily map directly to a particular service. Provider Services # Provider Services are meant to be the source of truth for explaining not only the pricing mechanics of the service but also to explain potentially nuanced concepts related to costs for that service. The focus of these pages is meant to be for the pricing of these services and not related to the actual management or orchestration of the service itself. Contributing # Anyone is welcome to contribute to the Cloud Cost Handbook by issuing a pull request on the GitHub repo. Contribute Slack Community # Generally interested in cloud concepts and associated costs? Join us in our public Slack community. We have a channel devoted to the handbook named \"cloud-cost-handbook\" where you're welcome to hang out, ask questions and/or spark conversation. Slack Community","title":"Overview"},{"location":"#cloud-cost-handbook","text":"The Cloud Cost Handbook is a free, open-source, community-supported set of guides meant to help explain often-times complex pricing of public cloud infrastructure and service providers in easy-to-understand terms. This guide is hosted on Github and is open to anyone to contribute their knowledge to the community. Vantage employees will maintain hosting the guide for everyone and ensure that content is relevant and adheres to styleguides.","title":"Cloud Cost Handbook"},{"location":"#structure","text":"This handbook is separated into two different sections for the time-being as explained below:","title":"Structure"},{"location":"#general-concepts","text":"These are general concepts that don't necessarily map directly to a particular service.","title":"General Concepts"},{"location":"#provider-services","text":"Provider Services are meant to be the source of truth for explaining not only the pricing mechanics of the service but also to explain potentially nuanced concepts related to costs for that service. The focus of these pages is meant to be for the pricing of these services and not related to the actual management or orchestration of the service itself.","title":"Provider Services"},{"location":"#contributing","text":"Anyone is welcome to contribute to the Cloud Cost Handbook by issuing a pull request on the GitHub repo. Contribute","title":"Contributing"},{"location":"#slack-community","text":"Generally interested in cloud concepts and associated costs? Join us in our public Slack community. We have a channel devoted to the handbook named \"cloud-cost-handbook\" where you're welcome to hang out, ask questions and/or spark conversation. Slack Community","title":"Slack Community"},{"location":"aws/concepts/autoscaling/","text":"The best way to optimize costs in the cloud is to not spend it in the first place. Enter Autoscaling. Autoscaling leverages the elasticity of the cloud to dynamically provision and remove capacity based on demand. That means that as demands decrease autoscaling will automatically scale down resources and allow you to save on costs accordingly. Autoscaling applies to a variety a variety of different services, some of which are described in more detail below. If you're looking for EC2 autoscaling concepts, please see the AWS EC2 service page for the autoscaling section . Application Autoscaling # For other resources in AWS, Application Autoscaling provides the ability to adjust provisioned resources. Application Autoscaling supports the following services: AppStream 2.0 fleets Aurora replicas Amazon Comprehend document classification and entity recognizer endpoints DynamoDB tables and global secondary indexes Amazon Elastic Container Service (ECS) services Amazon EMR clusters Amazon Keyspaces (for Apache Cassandra) tables Lambda function provisioned concurrency Amazon Managed Streaming for Apache Kafka (MSK) broker storage SageMaker endpoint variants Autoscaling Strategies # There are various methods by which autoscaling can occur. These are listed below in no partiuclar order: Target Scaling adds or removes capacity to keep a metric as near a specific value as possible. For example, target average CPU utilization of 50% across a set of ECS Tasks. If CPU utilization gets too high, add nodes. If CPU utilization gets too low, remove nodes. Step Scaling will adjust capacity up and down by dynamic amounts, depending on the magnitude of a metric. Scheduled Scaling will adjust mininmum and maximum capacity settings on a schedule. Simple Scaling will add or remove EC2 instances from an Autoscalng Group when an alarm is in alert state. Predictive Scaling can leverage historical metrics to preemptively scale EC2 workloads based on daily or weekly trends. Manual Scaling is possible with EC2 instances if teams need to intervene with an autoscaling group. This allows you to manually adjust the autoscaling target without any automation. Other Considerations # Adding capacity is generally an easy process. For compute, it's just a matter of launching new workers from static images or automated standup processes. Reducing capacity can be tricky depending on the application. Web applications generally have their requests clean up to prepare for termination within 30 seconds. Load balancers are often used to drain requests off instances and then terminate the instances \"cleanly\". Queue/batch workers, on the other hand, need to be done with their work, or stash their work somewhere before the node can be terminated. Otherwise, requests and/or data can be lost or incomplete. DynamoDB Provisioned Capacity has restrictions regarding how frequently it can be reduced (4 times per day at any time, plus any time when there hasn't been a reduction in the last hour). There are no restrictions regarding increasing capacity. Tables and Secondary Indexes are managed/scaled independently. Scaling cooldown can be the trickiest part of the process. It's generally best to aggressively scale up/out and conservatively scale down/in. A long cooldown process might be necessary when scaling out an application with a long startup process, but it can also block future scale out events, resulting in application instability. Scaling policies should be regularly evaluated and tuned. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Autoscaling"},{"location":"aws/concepts/autoscaling/#application-autoscaling","text":"For other resources in AWS, Application Autoscaling provides the ability to adjust provisioned resources. Application Autoscaling supports the following services: AppStream 2.0 fleets Aurora replicas Amazon Comprehend document classification and entity recognizer endpoints DynamoDB tables and global secondary indexes Amazon Elastic Container Service (ECS) services Amazon EMR clusters Amazon Keyspaces (for Apache Cassandra) tables Lambda function provisioned concurrency Amazon Managed Streaming for Apache Kafka (MSK) broker storage SageMaker endpoint variants","title":"Application Autoscaling"},{"location":"aws/concepts/autoscaling/#autoscaling-strategies","text":"There are various methods by which autoscaling can occur. These are listed below in no partiuclar order: Target Scaling adds or removes capacity to keep a metric as near a specific value as possible. For example, target average CPU utilization of 50% across a set of ECS Tasks. If CPU utilization gets too high, add nodes. If CPU utilization gets too low, remove nodes. Step Scaling will adjust capacity up and down by dynamic amounts, depending on the magnitude of a metric. Scheduled Scaling will adjust mininmum and maximum capacity settings on a schedule. Simple Scaling will add or remove EC2 instances from an Autoscalng Group when an alarm is in alert state. Predictive Scaling can leverage historical metrics to preemptively scale EC2 workloads based on daily or weekly trends. Manual Scaling is possible with EC2 instances if teams need to intervene with an autoscaling group. This allows you to manually adjust the autoscaling target without any automation.","title":"Autoscaling Strategies"},{"location":"aws/concepts/autoscaling/#other-considerations","text":"Adding capacity is generally an easy process. For compute, it's just a matter of launching new workers from static images or automated standup processes. Reducing capacity can be tricky depending on the application. Web applications generally have their requests clean up to prepare for termination within 30 seconds. Load balancers are often used to drain requests off instances and then terminate the instances \"cleanly\". Queue/batch workers, on the other hand, need to be done with their work, or stash their work somewhere before the node can be terminated. Otherwise, requests and/or data can be lost or incomplete. DynamoDB Provisioned Capacity has restrictions regarding how frequently it can be reduced (4 times per day at any time, plus any time when there hasn't been a reduction in the last hour). There are no restrictions regarding increasing capacity. Tables and Secondary Indexes are managed/scaled independently. Scaling cooldown can be the trickiest part of the process. It's generally best to aggressively scale up/out and conservatively scale down/in. A long cooldown process might be necessary when scaling out an application with a long startup process, but it can also block future scale out events, resulting in application instability. Scaling policies should be regularly evaluated and tuned. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Other Considerations"},{"location":"aws/concepts/bill/","text":"","title":"Bill"},{"location":"aws/concepts/budgets/","text":"","title":"Budgets"},{"location":"aws/concepts/credits/","text":"Most public cloud infastructure and service providers have a concept of credits. Credits are incentives typically given to customers opening up new accounts to attract them to build upon their platform. They allow you you to build, learn and get integrated into providers without have to spend money right from the beginning. Credit allotments usually are around $5,000 or $10,000 depending on the provider but can be as high as $100,000. Startup Credits Across Clouds # One strategy that is often used for especially cost-conscious startups for public cloud infrastructure providers who have the ability to easily move workloads is to receive credits from multiple providers and run workloads across different providers until credits expire across all of them. So for example, a startup may get $10,000 of AWS credits and $10,000 of GCP credits. A subset of customers will run their application on AWS until their $10,000 is completely utilized then migrate to GCP to use up $10,000 worth of credits there to get $20,000 in total free usage. Typically this is advised against because the operational overhead of running workloads across multiple clouds typically isn't worth it. The use-cases that this tends to work for is for very transferable or ephemeral workloads such as training models on GPUs or running containers with no associated state. Credit Expiration # It's important to note that credits typically have a lifecycle tied to them that causes them to expire. Oftentimes this catches customers by surprise. Usually credits are granted on a 1 year basis which means if you have remaining credits that aren't utilized by the expiration term, they're automatically removed from your account. It's important to keep track of your credit expiration dates as to not be caught off-guard. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Credits"},{"location":"aws/concepts/credits/#startup-credits-across-clouds","text":"One strategy that is often used for especially cost-conscious startups for public cloud infrastructure providers who have the ability to easily move workloads is to receive credits from multiple providers and run workloads across different providers until credits expire across all of them. So for example, a startup may get $10,000 of AWS credits and $10,000 of GCP credits. A subset of customers will run their application on AWS until their $10,000 is completely utilized then migrate to GCP to use up $10,000 worth of credits there to get $20,000 in total free usage. Typically this is advised against because the operational overhead of running workloads across multiple clouds typically isn't worth it. The use-cases that this tends to work for is for very transferable or ephemeral workloads such as training models on GPUs or running containers with no associated state.","title":"Startup Credits Across Clouds"},{"location":"aws/concepts/credits/#credit-expiration","text":"It's important to note that credits typically have a lifecycle tied to them that causes them to expire. Oftentimes this catches customers by surprise. Usually credits are granted on a 1 year basis which means if you have remaining credits that aren't utilized by the expiration term, they're automatically removed from your account. It's important to keep track of your credit expiration dates as to not be caught off-guard. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Credit Expiration"},{"location":"aws/concepts/regions/","text":"Pricing for public cloud infrastructure providers typically varies by geographic region. Depending on the nature of your applications, you may not have a choice but to be located as close to your users as possible for latency purposes. That being said, it is worth looking at pricing on a per region basis as there can be significant discounts on a per-region basis. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Regions"},{"location":"aws/concepts/reserved-instances/","text":"Reserved Instances (oftentimes referred to as their abbreviation of RIs) are one of the most popular and high-impact cost reduction methods you can leverage for cutting your bill. Reserved Instances give you the ability to pay upfront for certain AWS services to receive a discount. As a result, if you are able to profile usage across your AWS account and know that you'll hit certain usage levels, Reserved Instances can typically save you money. Reserved Instances are availabile to a variety of AWS services such as EC2 , ElastiCache and RDS . AWS Billing automatically applies your Reserved Instance discounted rate when attributes of your instance usage match attributes of an active Reserved Instance. For general compute usage (EC2, Fargate, etc.), Savings Plans are always preferred to Reserved Instances as they give you the same discount but are more flexible across all compute. It's important to note that Reserved Instances aren't actually separate instances. They are merely financial instruments that you buy and are automatically applied to your account. As a result, you can continue to spin up and use on-demand instances and purchase Reserved Instances concurrently. As on-demand instances match your Reserved Instance attributes, you'll automatically receive discounts. Reserved Instance Term # AWS gives different discounts depending on the term that you pay upfront for. You can yield greater savings for paying upfront for longer terms but lose flexibility as a result. We find that smaller customers just getting started in their infrastructure journey tend to prefer 1-Year Reserved Instances whereas more mature organizations will leverage 3-Year Reserved Instances for the greatest savings as they can more accurately model and predict their usage. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Reserved Instances"},{"location":"aws/concepts/reserved-instances/#reserved-instance-term","text":"AWS gives different discounts depending on the term that you pay upfront for. You can yield greater savings for paying upfront for longer terms but lose flexibility as a result. We find that smaller customers just getting started in their infrastructure journey tend to prefer 1-Year Reserved Instances whereas more mature organizations will leverage 3-Year Reserved Instances for the greatest savings as they can more accurately model and predict their usage. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Reserved Instance Term"},{"location":"aws/concepts/rightsizing/","text":"Rightsizing is a term used for identifying and augmenting certain resources for greater utilization and potential cost savings. Typically rightsizing occures when you're over-provisioned and can apply to a variety of services with some examples below: EC2 Instances : Oftentimes customers will choose one EC2 Instance that is over-allocated in terms the amount of vCPU and GB of RAM it is allocated. As a result, customers may be paying more on a per EC2-Instance basis. Customers who are able to identify opportunitites for rightsizing EC2 Instances can typically save significantly, especially if the EC2 Instance type chosen represents a large pool of instances. EBS Volumes : EBS Volumes are typically a large cost driver for many organizations and are often heavily under-utilized. EBS charges you for the amount of storage you have allocated versus utilize so its important to keep an eye on Volume utilization to rightsize and save accordinly. RDS Instances : RDS Instances are similar to EC2 Instances in that they're typically overprovisioned but rarely utilized appropriately. While RDS rightsizing can result in significant cost savings, databases tend to be one of the services that makes sense to leave overprovisioned that you can grow into as downtime for a database during a rightsizing process may not ultimatately be worth the organization cost. Container Services : ECS, Fargate and EKS allow you to run services of containers on a pool of underlying EC2 instances either managed by you or managed by AWS if you're using Fargate. Container Services are some of the hardest services to appropriately rightsize but can represent significant saving opportunities, especially for AWS Fargate. The first step in rightsizing is to have monitoring and observability in place to even know what your utilization is for these various services. Assuming you feel confident in your usage patterns and how they relate to utilization, your organization can begin to make some decision for potential area to rightsize. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Rightsizing"},{"location":"aws/concepts/savings-plans/","text":"Savings Plans are a flexible pricing model offering discounted prices compared to On-Demand pricing, in exchange for a specific usage commitment. Savings Plans are typically the highest impact, lowest effort way of realizing savings on your AWS account. They are roughly the same concept as Reserved Instances but offer greater flexiblity as they 1) can be utilized across multiple compute services (i.e., EC2 and Fargate) and 2) you aren't locked into a specific instance family. Similar to Reserved Instances, there are greater discounts for prepaying for a longer term. After purchasing a Savings Plan, AWS Billing will automatically apply savings as corresponding on-demand resources match the conditions of your Savings Plans. Savings Plans are only applicable to usage across Amazon EC2, AWS Lambda, and AWS Fargate. Typically, customers will use Savings Plans for these services and Reserved Instances for other services that aren't covered such as RDS and ElastiCache. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Savings Plans"},{"location":"aws/concepts/tags/","text":"Tags are one of the most powerful (though often overlooked) tools that can assist with your ability to observe and allocate cloud costs as it relates to public cloud infrastructure providers like AWS, Azure and GCP. While different AWS accounts can be useful for separating resources and costs across different environments (production, staging, qa, test, etc) or teams/business-units, tags are helpful for segmenting costs as it relates to your application. We encourage customers to adopt tagging strategies as early on at their organizations as possible. Similar to an effective unit testing suite, over time tags can give you confidence in understanding where your costs are coming from. Tags on AWS consist of two different parts: a key and a value . As a basic example you can imagine an example key with the value of \"service\" and potential value s of \"front-end\", \"back-end\", \"search\" or \"cache\". Upon assigning tags to resources, you can get greater visibility into where your costs are coming from. Instead of seeing how your costs are trending in aggregate, you can see how each part of your application is growing assuming you've leveraged tags correctly. Additionally, tags can be part of your existing workflows and are typically very easy to accomodate in infrastructure-as-code configuration files such as CloudFormation or Terraform. Activating Cost Allocation Tags # One of the more generally confusing experiences that customers experience on AWS is that tags are not incorporated into billing reports by default and need to be \"activated\". After you have assigned resources tags, here are the steps to \"activate\" the tags for them to be incorporated into billing data: To activate your tags Sign in to the AWS Management Console and open the Billing and Cost Management console at https://console.aws.amazon.com/billing/home?#/tags . In the navigation pane, choose Cost Allocation Tags. Select the tags that you want to activate. Choose Activate. After you create and apply tags to your resources, it can take up to 24 hours for the tags to appear in your reports. After you select your tags for activation, it can take up to 24 hours for tags to activate as well. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Tags"},{"location":"aws/concepts/tags/#activating-cost-allocation-tags","text":"One of the more generally confusing experiences that customers experience on AWS is that tags are not incorporated into billing reports by default and need to be \"activated\". After you have assigned resources tags, here are the steps to \"activate\" the tags for them to be incorporated into billing data: To activate your tags Sign in to the AWS Management Console and open the Billing and Cost Management console at https://console.aws.amazon.com/billing/home?#/tags . In the navigation pane, choose Cost Allocation Tags. Select the tags that you want to activate. Choose Activate. After you create and apply tags to your resources, it can take up to 24 hours for the tags to appear in your reports. After you select your tags for activation, it can take up to 24 hours for tags to activate as well. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Activating Cost Allocation Tags"},{"location":"aws/services/cloudfront-pricing/","text":"Amazon CloudFront Pricing Page Summary # Amazon CloudFront is a content delivery network (CDN) service used to distribute and cache traffic from one region to multiple geographic endpoints globally. Every CloudFront distribution includes an origin which is used to pull the original data from. An origin will typically be an S3 bucket or Load Balancer Endpoint. The traffic is distributed globally to speed up the access to an application which recieves visitors from across the globe. CloudFront Distributions are billed based on the amount of traffic they request from the origin, distribute out to the internet as well as per request processed. Distribution out to the internet is priced differently depending on the region which it is accessed. Regions are grouped into geographic regions. When creating a distribution it is possible to select which regions CloudFront will serve traffic from. Pricing Dimensions # Dimension Description Transfer Out to Internet Distributions are billed per GB of data transfered out of a geographic area to the internet. The prices are tiered and are lower the more traffic is transferred. Regional Data Transfer Out to Origin Distributions are billed per GB of data transfer from the distribution back to the origin. The prices are a flat-rate and dependent on their geographic area. Per Request Distributions are billed per 10,000 requests and are different rates based on whether the request is HTTP or HTTPS. If origin shield is configured there is an additional charge per 10,000 requests and are a standard rate regardless of protocol. Both are priced differently depending on geographic area. Origin Shield # Origin Shield can be enabled in order to reduce the amount of traffic being served directly from the origin. Origin shields are not available in every region. CloudFront Security Savings Bundle # The CloudFront Security Savings Bundle is a simple way to save up to 30% on the CloudFront charges on your AWS bill when you make a 1-year upfront commitment with no service-level configuration changes needed. You're billed in equal installments over the 12 months, starting from the time you purcahse the security savings bundle. Once you purchase the CloudFront Security Savings Bundle, the savings are automatically applied to your bill. If you're familiar with Savings Plans or Reserved Instances , this is essentially the CloudFront equivalent to those conceptually speaking. The reason for this being named a \"bundle\" is that by making this purchase you also get credits towards the AWS Web Application Firewall (WAF) service. Ten percent of the amount you pay in committed use for a CloudFront Security Savings Bundle will be granted toward AWS WAF. So for example if you pay $500 for a CloudFront Security Savings Bundle, $50 will also be applied towards AWS WAF. Custom Pricing # For customers who are willing to make certain minimum traffic commits (typically 10 TB/month or higher) they can contact AWS and negotiate custom discounted rates. CloudFront Versus Cloudflare # Cloudflare 1 is an edge network that offers a number of different performance, availability and security services. One of those services is an edge caching service that offer effectively the same service as Amazon CloudFront. The most important distinction between CloudFront and Cloudflare is not a technical differentiation but a business model differentiation. CloudFront utilizes a metered pricing model whereby you pay based on the amount of traffic that is served via the CloudFront service. 2 Cloudflare, on the otherhand, offers flat-rate pricing for its service without any bandwidth caps. 3 What this means is that as a customer of Cloudflare's Business plan , you can pay $200 per month and delivery unlimited traffic via the Cloudflare CDN. Seems too good to be true? Feel free to browse the official Cloudflare community where this question is asked and answered multiple times. Considerations # Price is not the only consideration that goes into making a decision about whether to utilize CloudFront or a competing CDN service. Performance, availability, user experience, support and legal compliance are other factors that will factor into the decision to utilize one service over another. Availability # In order to offer customers unlimited bandwidth, Cloudflare utilizes service degradation based on their plan levels to prioritize higher tier customers in the event of a service degredation. The two most common service degradations for Cloudflare are either a DDoS attack that is overwhelming one or more points-of-presence (PoP) in the network or a legitimate surge in traffic due to any number of events. When the resources for a PoP are being depleted and service is being degraded, Cloudflare will choose to route traffic for customers out of that location based on the plan level they are subscribed to. Free traffic will be routed away from the PoP first, then Pro, Business, etc. The effect of having traffic routed out of a specific PoP is that users that are closest to the PoP will have some level of service degredation since they will instead have their traffic served from a PoP that is farther away than their most ideal PoP. In locations where the next nearest available PoP is close this degredation will be practically unnoticable. In locations where the next available PoP is topologically distant service degredation can potentially be significant. Technical # In the scenario that you are utilizing CloudFront and have an Amazon service designated as the origin for the content being served, typically this would be an S3 bucket or maybe EC2 with an attached EBS volume, you should consider that by switch from CloudFront as your CDN to Cloudflare you will incur egress charges for data transfer from AWS to Cloudflare. AWS does not charge customers any egress fees when moving content from an AWS service like S3 or EC2 to CloudFront. 4 The amount of charges will largely be dependent on your particular services cache hit ratio. The higher the cache hit ratio, the less cache misses that will incur AWS egress charges. This practice favors pairing CloudFront with an AWS service as origin. That being said, for most customers with significant CloudFront traffic they will still come out on top by considering a flat-rate priced CDN plan. On top of this, you can also consider moving your content off of an AWS service to a provider in the Bandwidth Alliance . By utilizing the Cloudflare CDN service and a Bandwidth Alliance partner as the content origin, you can take advantage of the flat-rate pricing of the Cloudflare self-serve plans and eliminate all egress costs between Cloudflare and your origin provider of choice. This effectively gives you the same benefit that AWS offers customer of no egress charges between an AWS service and CloudFront but with the power of the flat-rate pricing that is available via the Cloudflare self-serve plans. Further details can be found at in the S3 service article of the Cloud Cost Handbook. Sales # Another side effect of subscribing to a self-serve plan from Cloudflare is that users of these plans are used as part of the sales funnel for the Cloudflare sales team. What this means is that by signing up for Cloudflare you are giving your contact information that can be utilized by the sales team in order for them to contact you about other Cloudflare services and offerings. The important thing to remember is that, as long as you aren't breaking the Cloudflare Terms of Service (ToS) they cannot force you to purchase any additional services. Contribute Help us improve this page by making a contribution on our Github repository . This guide is calling special attention to Cloudflare and no other vendors in this space due to the unique offerings that Cloudflare has that no other provider offers. Specifically, that they offer self-serve plans with flat-rate pricing and no bandwidth caps. If you are aware of any other services with a similar offering, please submit an issue or pull request and we will update the guide. \u21a9 Direct link to CloudFront pricing that details metered pricing model: https://aws.amazon.com/cloudfront/pricing/ \u21a9 Direct link to the Cloudflare Terms of Service for the self-serve plans (i.e. the Free, Pro and Business plans):https://www.cloudflare.com/terms/ \u21a9 \"If you are using an AWS origin, effective December 1, 2014, data transferred from origin to edge locations (Amazon CloudFront \"origin fetches\") will be free of charge.\" https://aws.amazon.com/cloudfront/pricing/ \u21a9","title":"CloudFront"},{"location":"aws/services/cloudfront-pricing/#summary","text":"Amazon CloudFront is a content delivery network (CDN) service used to distribute and cache traffic from one region to multiple geographic endpoints globally. Every CloudFront distribution includes an origin which is used to pull the original data from. An origin will typically be an S3 bucket or Load Balancer Endpoint. The traffic is distributed globally to speed up the access to an application which recieves visitors from across the globe. CloudFront Distributions are billed based on the amount of traffic they request from the origin, distribute out to the internet as well as per request processed. Distribution out to the internet is priced differently depending on the region which it is accessed. Regions are grouped into geographic regions. When creating a distribution it is possible to select which regions CloudFront will serve traffic from.","title":"Summary"},{"location":"aws/services/cloudfront-pricing/#pricing-dimensions","text":"Dimension Description Transfer Out to Internet Distributions are billed per GB of data transfered out of a geographic area to the internet. The prices are tiered and are lower the more traffic is transferred. Regional Data Transfer Out to Origin Distributions are billed per GB of data transfer from the distribution back to the origin. The prices are a flat-rate and dependent on their geographic area. Per Request Distributions are billed per 10,000 requests and are different rates based on whether the request is HTTP or HTTPS. If origin shield is configured there is an additional charge per 10,000 requests and are a standard rate regardless of protocol. Both are priced differently depending on geographic area.","title":"Pricing Dimensions"},{"location":"aws/services/cloudfront-pricing/#origin-shield","text":"Origin Shield can be enabled in order to reduce the amount of traffic being served directly from the origin. Origin shields are not available in every region.","title":"Origin Shield"},{"location":"aws/services/cloudfront-pricing/#cloudfront-security-savings-bundle","text":"The CloudFront Security Savings Bundle is a simple way to save up to 30% on the CloudFront charges on your AWS bill when you make a 1-year upfront commitment with no service-level configuration changes needed. You're billed in equal installments over the 12 months, starting from the time you purcahse the security savings bundle. Once you purchase the CloudFront Security Savings Bundle, the savings are automatically applied to your bill. If you're familiar with Savings Plans or Reserved Instances , this is essentially the CloudFront equivalent to those conceptually speaking. The reason for this being named a \"bundle\" is that by making this purchase you also get credits towards the AWS Web Application Firewall (WAF) service. Ten percent of the amount you pay in committed use for a CloudFront Security Savings Bundle will be granted toward AWS WAF. So for example if you pay $500 for a CloudFront Security Savings Bundle, $50 will also be applied towards AWS WAF.","title":"CloudFront Security Savings Bundle"},{"location":"aws/services/cloudfront-pricing/#custom-pricing","text":"For customers who are willing to make certain minimum traffic commits (typically 10 TB/month or higher) they can contact AWS and negotiate custom discounted rates.","title":"Custom Pricing"},{"location":"aws/services/cloudfront-pricing/#cloudfront-versus-cloudflare","text":"Cloudflare 1 is an edge network that offers a number of different performance, availability and security services. One of those services is an edge caching service that offer effectively the same service as Amazon CloudFront. The most important distinction between CloudFront and Cloudflare is not a technical differentiation but a business model differentiation. CloudFront utilizes a metered pricing model whereby you pay based on the amount of traffic that is served via the CloudFront service. 2 Cloudflare, on the otherhand, offers flat-rate pricing for its service without any bandwidth caps. 3 What this means is that as a customer of Cloudflare's Business plan , you can pay $200 per month and delivery unlimited traffic via the Cloudflare CDN. Seems too good to be true? Feel free to browse the official Cloudflare community where this question is asked and answered multiple times.","title":"CloudFront Versus Cloudflare"},{"location":"aws/services/cloudfront-pricing/#considerations","text":"Price is not the only consideration that goes into making a decision about whether to utilize CloudFront or a competing CDN service. Performance, availability, user experience, support and legal compliance are other factors that will factor into the decision to utilize one service over another.","title":"Considerations"},{"location":"aws/services/cloudfront-pricing/#availability","text":"In order to offer customers unlimited bandwidth, Cloudflare utilizes service degradation based on their plan levels to prioritize higher tier customers in the event of a service degredation. The two most common service degradations for Cloudflare are either a DDoS attack that is overwhelming one or more points-of-presence (PoP) in the network or a legitimate surge in traffic due to any number of events. When the resources for a PoP are being depleted and service is being degraded, Cloudflare will choose to route traffic for customers out of that location based on the plan level they are subscribed to. Free traffic will be routed away from the PoP first, then Pro, Business, etc. The effect of having traffic routed out of a specific PoP is that users that are closest to the PoP will have some level of service degredation since they will instead have their traffic served from a PoP that is farther away than their most ideal PoP. In locations where the next nearest available PoP is close this degredation will be practically unnoticable. In locations where the next available PoP is topologically distant service degredation can potentially be significant.","title":"Availability"},{"location":"aws/services/cloudfront-pricing/#technical","text":"In the scenario that you are utilizing CloudFront and have an Amazon service designated as the origin for the content being served, typically this would be an S3 bucket or maybe EC2 with an attached EBS volume, you should consider that by switch from CloudFront as your CDN to Cloudflare you will incur egress charges for data transfer from AWS to Cloudflare. AWS does not charge customers any egress fees when moving content from an AWS service like S3 or EC2 to CloudFront. 4 The amount of charges will largely be dependent on your particular services cache hit ratio. The higher the cache hit ratio, the less cache misses that will incur AWS egress charges. This practice favors pairing CloudFront with an AWS service as origin. That being said, for most customers with significant CloudFront traffic they will still come out on top by considering a flat-rate priced CDN plan. On top of this, you can also consider moving your content off of an AWS service to a provider in the Bandwidth Alliance . By utilizing the Cloudflare CDN service and a Bandwidth Alliance partner as the content origin, you can take advantage of the flat-rate pricing of the Cloudflare self-serve plans and eliminate all egress costs between Cloudflare and your origin provider of choice. This effectively gives you the same benefit that AWS offers customer of no egress charges between an AWS service and CloudFront but with the power of the flat-rate pricing that is available via the Cloudflare self-serve plans. Further details can be found at in the S3 service article of the Cloud Cost Handbook.","title":"Technical"},{"location":"aws/services/cloudfront-pricing/#sales","text":"Another side effect of subscribing to a self-serve plan from Cloudflare is that users of these plans are used as part of the sales funnel for the Cloudflare sales team. What this means is that by signing up for Cloudflare you are giving your contact information that can be utilized by the sales team in order for them to contact you about other Cloudflare services and offerings. The important thing to remember is that, as long as you aren't breaking the Cloudflare Terms of Service (ToS) they cannot force you to purchase any additional services. Contribute Help us improve this page by making a contribution on our Github repository . This guide is calling special attention to Cloudflare and no other vendors in this space due to the unique offerings that Cloudflare has that no other provider offers. Specifically, that they offer self-serve plans with flat-rate pricing and no bandwidth caps. If you are aware of any other services with a similar offering, please submit an issue or pull request and we will update the guide. \u21a9 Direct link to CloudFront pricing that details metered pricing model: https://aws.amazon.com/cloudfront/pricing/ \u21a9 Direct link to the Cloudflare Terms of Service for the self-serve plans (i.e. the Free, Pro and Business plans):https://www.cloudflare.com/terms/ \u21a9 \"If you are using an AWS origin, effective December 1, 2014, data transferred from origin to edge locations (Amazon CloudFront \"origin fetches\") will be free of charge.\" https://aws.amazon.com/cloudfront/pricing/ \u21a9","title":"Sales"},{"location":"aws/services/cloudwatch-pricing/","text":"Amazon CloudWatch Pricing Page Summary # Amazon CloudWatch is a logging, monitoring and observability service. As with most monitoring/observibility tools the cost of service is based on the amount of data that is collected and stored as well as a number of other factors. CloudWatch is no different. For most use-cases, the largest CloudWatch costs are made up of the number of metrics and logs that a user is ingesting and storing. Oftentimes CloudWatch is leveraged automatically by other AWS services for metric and log storage. Users are sometimes surprised when they spin up a number of unrelated services which they accounted for during planning but are then greeted with an accompanying spike in CloudWatch costs that they didn't account for. CloudWatch stores and process data from an umbrella of different AWS services which means that sometimes it isn't obvious why the overall CloudWatch bill has increased. Diving into subcategory costs can help shed light on which other AWS services are causing CloudWatch costs to increase. Pricing Dimensions # Dimension Description Custom Metric Storage AWS charges you for the number of custom metrics you store with them per month. CloudWatch's unit pricing is progressive; the first 10,000 metrics tracked is $0.30 per metric per month, the next 240,000 costs $0.10 and so on. 1 This gives users with large number of metrics automatic economies of scale as they grow the number of metrics tracked. Note: Pricing is dependant on the region where you store your metrics. 2 CloudWatch API Requests AWS charges you for the following API requests: GetMetricData , GetInsightRuleReport , GetMetricWidgetImage , GetMetricStatistics , ListMetrics , PutMetricData , GetDashboard , ListDashboards , PutDashboard and DeleteDashboards . For most AWS services there is no API charge for sending metrics. A user would normally be charged by the CloudWatch API for ingesting metrics from a non-AWS service or for a third-party infrastructure monitoring/observibility tool reading from the API to collect metrics. Pricing is dependant on the region where CloudWatch is deployed. 3 CloudWatch Dashboards AWS charges $3.00 per month per CloudWatch dashboard. CloudWatch Alarms CloudWatch alarms are priced based on the resolution of the alarm (60 seconds versus 10 seconds) and if you need to combine multiple alarms together into a more complex alarm like anomoly detection or a composite alarm. Pricing is dependant on the region where CloudWatch is deployed. 3 CloudWatch Logs AWS charges you for two components as it relates to CloudWatch Logs: (1) ingestion and (2) storage. CloudWatch Events AWS charges you for CloudWatch Events which are changes in your AWS environment. For example, you can trigger an event whenever an EC2 instance is created. You are charged a rate per one million events. CloudWatch Contributor Insights Contributor Insights are only available for CloudWatch Logs and DynamoDB. For CloudWatch Logs, Contributor Insights are priced per-rule per-month, and for every million log events per month that match your rule. For DynamoDB, Contributor Insights are priced per-rule per-month and for every million DynamoDB Events, which occur when items are read from or written to your DynamoDB table. CloudWatch Canaries Canaries are priced based on the number of runs. Pricing is very specific to region. Be sure to check where you are running your CloudWatch Canaries to be aware of the price for that region. Contribute Help us improve this page by making a contribution on our Github repository . Price is based on US East (Ohio) region as of July 28, 2021. See footnote below for comment about pricing per region. \u21a9 Or at least this is what the CloudWatch pricing page states. If you click through all of the regions the prices are all the same for custom metric storage as of July 28, 2021. \u21a9 Pircing is fairly uniform across regions. Special regions like Sao Paulo and GovCloud diverge from the standard pricing. \u21a9 \u21a9","title":"CloudWatch"},{"location":"aws/services/cloudwatch-pricing/#summary","text":"Amazon CloudWatch is a logging, monitoring and observability service. As with most monitoring/observibility tools the cost of service is based on the amount of data that is collected and stored as well as a number of other factors. CloudWatch is no different. For most use-cases, the largest CloudWatch costs are made up of the number of metrics and logs that a user is ingesting and storing. Oftentimes CloudWatch is leveraged automatically by other AWS services for metric and log storage. Users are sometimes surprised when they spin up a number of unrelated services which they accounted for during planning but are then greeted with an accompanying spike in CloudWatch costs that they didn't account for. CloudWatch stores and process data from an umbrella of different AWS services which means that sometimes it isn't obvious why the overall CloudWatch bill has increased. Diving into subcategory costs can help shed light on which other AWS services are causing CloudWatch costs to increase.","title":"Summary"},{"location":"aws/services/cloudwatch-pricing/#pricing-dimensions","text":"Dimension Description Custom Metric Storage AWS charges you for the number of custom metrics you store with them per month. CloudWatch's unit pricing is progressive; the first 10,000 metrics tracked is $0.30 per metric per month, the next 240,000 costs $0.10 and so on. 1 This gives users with large number of metrics automatic economies of scale as they grow the number of metrics tracked. Note: Pricing is dependant on the region where you store your metrics. 2 CloudWatch API Requests AWS charges you for the following API requests: GetMetricData , GetInsightRuleReport , GetMetricWidgetImage , GetMetricStatistics , ListMetrics , PutMetricData , GetDashboard , ListDashboards , PutDashboard and DeleteDashboards . For most AWS services there is no API charge for sending metrics. A user would normally be charged by the CloudWatch API for ingesting metrics from a non-AWS service or for a third-party infrastructure monitoring/observibility tool reading from the API to collect metrics. Pricing is dependant on the region where CloudWatch is deployed. 3 CloudWatch Dashboards AWS charges $3.00 per month per CloudWatch dashboard. CloudWatch Alarms CloudWatch alarms are priced based on the resolution of the alarm (60 seconds versus 10 seconds) and if you need to combine multiple alarms together into a more complex alarm like anomoly detection or a composite alarm. Pricing is dependant on the region where CloudWatch is deployed. 3 CloudWatch Logs AWS charges you for two components as it relates to CloudWatch Logs: (1) ingestion and (2) storage. CloudWatch Events AWS charges you for CloudWatch Events which are changes in your AWS environment. For example, you can trigger an event whenever an EC2 instance is created. You are charged a rate per one million events. CloudWatch Contributor Insights Contributor Insights are only available for CloudWatch Logs and DynamoDB. For CloudWatch Logs, Contributor Insights are priced per-rule per-month, and for every million log events per month that match your rule. For DynamoDB, Contributor Insights are priced per-rule per-month and for every million DynamoDB Events, which occur when items are read from or written to your DynamoDB table. CloudWatch Canaries Canaries are priced based on the number of runs. Pricing is very specific to region. Be sure to check where you are running your CloudWatch Canaries to be aware of the price for that region. Contribute Help us improve this page by making a contribution on our Github repository . Price is based on US East (Ohio) region as of July 28, 2021. See footnote below for comment about pricing per region. \u21a9 Or at least this is what the CloudWatch pricing page states. If you click through all of the regions the prices are all the same for custom metric storage as of July 28, 2021. \u21a9 Pircing is fairly uniform across regions. Special regions like Sao Paulo and GovCloud diverge from the standard pricing. \u21a9 \u21a9","title":"Pricing Dimensions"},{"location":"aws/services/dynamodb-pricing/","text":"Amazon DynamoDB Pricing Page Summary # DynamoDB is Amazon's primary managed NoSQL database service. It offers single-digit-millisecond latency, scales to effectively unlimited requests-per-second & storage, and has (largely) predictable pricing. DynamoDB, like most NoSQL datastores, differs substantially from relational databases - it can only be queried via primary key attributes on the base table & indexes. Pricing Dimensions # Dimension Options Description Docs Billing Mode On Demand, Provisioned Throughput Choose between paying per read/write or per allocated requests-per-second-throughput Docs Write Type Standard, Transactional Transactional operations allow ACID guarantees at twice the standard cost Docs Read Type Eventually Consistent, Strongly Consistent, Transactional Dynamo reads are by default Eventually Consistent - when you read from a table, the response might not reflect the results of a recently completed write. Docs Read Operation GetItem, Scan, Query Scans return the entire contents of a table; Queries allow a much faster & cheaper read of a subsection of the table Docs Indexes None, Local Secondary Index, Global Secondary Index Indexes allow an alternate set of partition_key + sort_key to be used for queries Overview Billing Mode # \"Use on-demand until it hurts\" - Alex DeBrie, quoting Jared Short Provisioned Throughput is cheaper if you have a meaningful number of reads/writes distributed evenly across time . Any reads/writes above the provisioned threshold will fail , so it is not well suited to bursty or unpredictable workloads. Provisioned Throughput includes optional Auto-Scaling if throughput thresholds are being exceeded ( Docs ). AWS Free Tier includes 25 reads/writes-per-second of Provisioned Throughput (across any # of tables), but does not include any On Demand mode usage. Billing Mode Unit Unit Definition On Demand Read Request Unit (RRU) read two <4KB items, eventually consistent On Demand Write Request Unit (WRU) write one <1KB item Provisioned Throughput Read Capacity Unit (RCU) two reads per second (<4KB items), eventually consistent Provisioned Throughput Write Capacity Unit (WCU) one write per second (<1KB item) Write Type # Standard writes are relatively straightforward and include single item writes ( table.put_item ) and batch writes ( batch.put_item ). Transactions ( client.execute_transaction ) group up to 25 writes (or reads, updates, or deletes) together and guarantee that they succeed or fail together. For a given write of an item up to 1 KB in size: Type Cost Standard single item 1 WRU Standard batch 1 WRU (per item) Transactional 2 WRU (2x) Oversize 4 KB item 4 WRU (4x, size dependent) Read Type # Part of what makes DynamoDB a compelling offering is its hybrid approach to the CAP theorem 1 - it can adjust between eventually and strongly consistent as needed. Wherever acceptable to the business needs and current data modeling, it is faster and cheaper to use eventually consistent reads. That said, some business logic unequivocally dictates strongly consistent reads (for example: an ATM reading a customer's balance). For a given read of an item up to 4 KB in size: Type Cost Eventually Consistent 0.5 RRU Strongly Consistent 1 RRU (2x) Transactional 2 RRU (4x) Read Operation # Getting a single item is as simple as providing its partition_key (and sort_key , if the table has one) Queries, however, are much more involved. NoSQL databases like DynamoDB can require significant upfront data modeling work to enable the query flexibility that SQL-based databases have by default. Scans require reading the entire table, and are correspondingly slow and expensive. Wherever possible, avoid scanning Dynamo tables. Type Cost GetItem 1 RRU Query {# of items meeting query logic} RRU Scan {# of items in table} RRU Indexes # Every DynamoDB table has a partition_key ( pk ) and optional sort_key ( sk ) specified at the time of creation. Indexes allow alternate partition and sort keys to be used to query items. They may be created at any time and are automatically maintained as new items are written. Indexes can help control costs in two primary ways: Queries on a new index return less unnecessary items (than the alternative/existing query) and thus cost less RRUs. Each index optionally allows a subset of item attributes to be projected to that index. Projecting a subset can save on read costs if items are regularly >4 KB, but the projected attribute names+values sum to <4KB. 2 Type Primary Key Attributes Base table Initial pk + optional sk Local Secondary Index (LSI) Initial pk + different sk Global Secondary Index (GSI) Different pk + optional different sk Info The provisioned throughput settings of a global secondary index are separate from those of its base table Other # DynamoDB tables can optionally enforce a Time To Live (TTL) on items in the table, such that they expire after that amount of time (guaranteed within +48 hours). Dynamo exposes the time-ordered sequence of item-level changes on a given table via DynamoDB Streams . Reading change-data from Streams is slightly cheaper per request than reading the table itself (on pay per use BillingMode). The first 2.5M reads per month are free. Further Reading # An overview of the architecture of DynamoDB can be found in the DynamoDB Paper You can, if you so choose, use a SQL-like syntax to interface with DynamoDB via the recently-launched PartiSQL support Contribute Help us improve this page by making a contribution on our Github repository . The CAP Theorem is a computer science theorem that observes that a distributed datastore cannot guarantee all three of Consistency, Availability, and Partition Tolerance. \u21a9 If a query to an index with projection requests attribute values not in the projected values, it will incur twice the normal read cost, as the remaining attribute values must be fetched from the base table \u21a9","title":"DynamoDB"},{"location":"aws/services/dynamodb-pricing/#summary","text":"DynamoDB is Amazon's primary managed NoSQL database service. It offers single-digit-millisecond latency, scales to effectively unlimited requests-per-second & storage, and has (largely) predictable pricing. DynamoDB, like most NoSQL datastores, differs substantially from relational databases - it can only be queried via primary key attributes on the base table & indexes.","title":"Summary"},{"location":"aws/services/dynamodb-pricing/#pricing-dimensions","text":"Dimension Options Description Docs Billing Mode On Demand, Provisioned Throughput Choose between paying per read/write or per allocated requests-per-second-throughput Docs Write Type Standard, Transactional Transactional operations allow ACID guarantees at twice the standard cost Docs Read Type Eventually Consistent, Strongly Consistent, Transactional Dynamo reads are by default Eventually Consistent - when you read from a table, the response might not reflect the results of a recently completed write. Docs Read Operation GetItem, Scan, Query Scans return the entire contents of a table; Queries allow a much faster & cheaper read of a subsection of the table Docs Indexes None, Local Secondary Index, Global Secondary Index Indexes allow an alternate set of partition_key + sort_key to be used for queries Overview","title":"Pricing Dimensions"},{"location":"aws/services/dynamodb-pricing/#billing-mode","text":"\"Use on-demand until it hurts\" - Alex DeBrie, quoting Jared Short Provisioned Throughput is cheaper if you have a meaningful number of reads/writes distributed evenly across time . Any reads/writes above the provisioned threshold will fail , so it is not well suited to bursty or unpredictable workloads. Provisioned Throughput includes optional Auto-Scaling if throughput thresholds are being exceeded ( Docs ). AWS Free Tier includes 25 reads/writes-per-second of Provisioned Throughput (across any # of tables), but does not include any On Demand mode usage. Billing Mode Unit Unit Definition On Demand Read Request Unit (RRU) read two <4KB items, eventually consistent On Demand Write Request Unit (WRU) write one <1KB item Provisioned Throughput Read Capacity Unit (RCU) two reads per second (<4KB items), eventually consistent Provisioned Throughput Write Capacity Unit (WCU) one write per second (<1KB item)","title":"Billing Mode"},{"location":"aws/services/dynamodb-pricing/#write-type","text":"Standard writes are relatively straightforward and include single item writes ( table.put_item ) and batch writes ( batch.put_item ). Transactions ( client.execute_transaction ) group up to 25 writes (or reads, updates, or deletes) together and guarantee that they succeed or fail together. For a given write of an item up to 1 KB in size: Type Cost Standard single item 1 WRU Standard batch 1 WRU (per item) Transactional 2 WRU (2x) Oversize 4 KB item 4 WRU (4x, size dependent)","title":"Write Type"},{"location":"aws/services/dynamodb-pricing/#read-type","text":"Part of what makes DynamoDB a compelling offering is its hybrid approach to the CAP theorem 1 - it can adjust between eventually and strongly consistent as needed. Wherever acceptable to the business needs and current data modeling, it is faster and cheaper to use eventually consistent reads. That said, some business logic unequivocally dictates strongly consistent reads (for example: an ATM reading a customer's balance). For a given read of an item up to 4 KB in size: Type Cost Eventually Consistent 0.5 RRU Strongly Consistent 1 RRU (2x) Transactional 2 RRU (4x)","title":"Read Type"},{"location":"aws/services/dynamodb-pricing/#read-operation","text":"Getting a single item is as simple as providing its partition_key (and sort_key , if the table has one) Queries, however, are much more involved. NoSQL databases like DynamoDB can require significant upfront data modeling work to enable the query flexibility that SQL-based databases have by default. Scans require reading the entire table, and are correspondingly slow and expensive. Wherever possible, avoid scanning Dynamo tables. Type Cost GetItem 1 RRU Query {# of items meeting query logic} RRU Scan {# of items in table} RRU","title":"Read Operation"},{"location":"aws/services/dynamodb-pricing/#indexes","text":"Every DynamoDB table has a partition_key ( pk ) and optional sort_key ( sk ) specified at the time of creation. Indexes allow alternate partition and sort keys to be used to query items. They may be created at any time and are automatically maintained as new items are written. Indexes can help control costs in two primary ways: Queries on a new index return less unnecessary items (than the alternative/existing query) and thus cost less RRUs. Each index optionally allows a subset of item attributes to be projected to that index. Projecting a subset can save on read costs if items are regularly >4 KB, but the projected attribute names+values sum to <4KB. 2 Type Primary Key Attributes Base table Initial pk + optional sk Local Secondary Index (LSI) Initial pk + different sk Global Secondary Index (GSI) Different pk + optional different sk Info The provisioned throughput settings of a global secondary index are separate from those of its base table","title":"Indexes"},{"location":"aws/services/dynamodb-pricing/#other","text":"DynamoDB tables can optionally enforce a Time To Live (TTL) on items in the table, such that they expire after that amount of time (guaranteed within +48 hours). Dynamo exposes the time-ordered sequence of item-level changes on a given table via DynamoDB Streams . Reading change-data from Streams is slightly cheaper per request than reading the table itself (on pay per use BillingMode). The first 2.5M reads per month are free.","title":"Other"},{"location":"aws/services/dynamodb-pricing/#further-reading","text":"An overview of the architecture of DynamoDB can be found in the DynamoDB Paper You can, if you so choose, use a SQL-like syntax to interface with DynamoDB via the recently-launched PartiSQL support Contribute Help us improve this page by making a contribution on our Github repository . The CAP Theorem is a computer science theorem that observes that a distributed datastore cannot guarantee all three of Consistency, Availability, and Partition Tolerance. \u21a9 If a query to an index with projection requests attribute values not in the projected values, it will incur twice the normal read cost, as the remaining attribute values must be fetched from the base table \u21a9","title":"Further Reading"},{"location":"aws/services/ebs-pricing/","text":"Amazon EBS Pricing Page Summary # Amazon Elastic Block Storage (EBS) is amazon's block storage offering that allows you to create \"Volumes\" which is the base primitive of everything related to EBS. There are multiple EBS \"Volume Types\" that offer different capabilites and have their own set of pricing. EBS costs are factored into the cost category of \"EC2-Other\" on your AWS bill which can oftentimes complicate understanding where these costs are coming from. It is important to note that you are charged for the amount of provisioned storage not utilized storage. So, for example, if you create a 20GB EBS Volume and only utilize 1GB of it, you are still charged for all 20GB. Pricing Dimensions # Dimension Description Volume Storage Hours When you create an EBS Volume you allocate a certain amount of storage to it. Ultimately the main cost of an EBS Volume is the result of the amount of hours you're using an EBS Volume and the size you allocate. Volume Type EBS has different types of Volume Types which are documented below. Each Volume Type has different rates. Provisioned IOPS Certain EBS Volume types (io2, io2) allow you to specific an amount of provisioned input/output operations per second which is abbreviated as IOPS and pronounced as \"eye-ops\". When using these volume types you are charged for the amount of provisioned iops even if you don't fully utilize them. Amazon EBS Snapshots Amazon EBS Snapshots are a point in time copy of your block volume data. EBS Snapshots are stored incrementally, which means you are billed only for the changed blocks stored. EBS Snapshot API Requests EBS charges you for the amount of API calls you make for snapshots. These are charged in increments of thousands of API requests. Volume Types # Amazon EBS offers a few different Volume Types that have different pricing rates and functionality. Each EBS Volume Type is described below: Volume Type Description General Purpose SSD (gp2, gp3) General Purpose SSD (gp3) volumes offer cost-effective storage that is ideal for a broad range of workloads. Provisioned IOPS (io1, io2) Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. Stranded Volumes # Oftentimes, EBS Volumes are created in conjunction with other AWS resources such as EC2 instances but are de-coupled from the lifecycle of those other resources. One common pattern we see is that developers will create EC2 instances with EBS Volumes attached but when they delete the EC2 instance, they assume that the EBS Volume is destroyed accordingly. In larger-scale environments with autoscaling this problem can grow significantly as a part of an AWS bill. We recommend that you periodically profile for unattached or stranded EBS Volumes. Contribute Help us improve this page by making a contribution on our Github repository .","title":"EBS"},{"location":"aws/services/ebs-pricing/#summary","text":"Amazon Elastic Block Storage (EBS) is amazon's block storage offering that allows you to create \"Volumes\" which is the base primitive of everything related to EBS. There are multiple EBS \"Volume Types\" that offer different capabilites and have their own set of pricing. EBS costs are factored into the cost category of \"EC2-Other\" on your AWS bill which can oftentimes complicate understanding where these costs are coming from. It is important to note that you are charged for the amount of provisioned storage not utilized storage. So, for example, if you create a 20GB EBS Volume and only utilize 1GB of it, you are still charged for all 20GB.","title":"Summary"},{"location":"aws/services/ebs-pricing/#pricing-dimensions","text":"Dimension Description Volume Storage Hours When you create an EBS Volume you allocate a certain amount of storage to it. Ultimately the main cost of an EBS Volume is the result of the amount of hours you're using an EBS Volume and the size you allocate. Volume Type EBS has different types of Volume Types which are documented below. Each Volume Type has different rates. Provisioned IOPS Certain EBS Volume types (io2, io2) allow you to specific an amount of provisioned input/output operations per second which is abbreviated as IOPS and pronounced as \"eye-ops\". When using these volume types you are charged for the amount of provisioned iops even if you don't fully utilize them. Amazon EBS Snapshots Amazon EBS Snapshots are a point in time copy of your block volume data. EBS Snapshots are stored incrementally, which means you are billed only for the changed blocks stored. EBS Snapshot API Requests EBS charges you for the amount of API calls you make for snapshots. These are charged in increments of thousands of API requests.","title":"Pricing Dimensions"},{"location":"aws/services/ebs-pricing/#volume-types","text":"Amazon EBS offers a few different Volume Types that have different pricing rates and functionality. Each EBS Volume Type is described below: Volume Type Description General Purpose SSD (gp2, gp3) General Purpose SSD (gp3) volumes offer cost-effective storage that is ideal for a broad range of workloads. Provisioned IOPS (io1, io2) Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.","title":"Volume Types"},{"location":"aws/services/ebs-pricing/#stranded-volumes","text":"Oftentimes, EBS Volumes are created in conjunction with other AWS resources such as EC2 instances but are de-coupled from the lifecycle of those other resources. One common pattern we see is that developers will create EC2 instances with EBS Volumes attached but when they delete the EC2 instance, they assume that the EBS Volume is destroyed accordingly. In larger-scale environments with autoscaling this problem can grow significantly as a part of an AWS bill. We recommend that you periodically profile for unattached or stranded EBS Volumes. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Stranded Volumes"},{"location":"aws/services/ec2-other-pricing/","text":"Summary # EC2-Other is a category of AWS costs that typically causes the greatest amount of confusion for customers as it doesn't necessarily map to a single AWS service. EC2-Other encompasses the following costs: Usage Type Description EBS Volume Usage Usage for EBS Volumes . EBS Snapshot Usage Usage for EBS Snapshots . CPU Credits from t2/t3/t4g EC2 instances T-family EC2 Instances can carry potential CPU credit charges as described more below. NAT Gateway Usage Hourly usage for NAT Gateways. Data Transfer Idle Elastic IP Address usage AWS charges you for unattached IP addresses. It's typically good hygiene to occasionally monitor for stranded resources and clean them up. Stranded Resources # Unused or stranded EBS Volumes and IP Addresses can add up over time especially if these resources are created automatically as part of an autoscaling service where they're spun up but not down. You should consider occasionally auditing your unattached EBS Volumes and IP addresses to see if you can clean them up to save costs. What are t2/t3/T4g CPU credit charges? # T2, T3 and T4g instances have a concept of \"Unlimited mode\" whereby you are charged a per-vCPU hour for bursting into this CPU usage. If you are leveraging these EC2 Instance Types with unlimited mode enabled, you should consider keeping an eye on these costs. Depending on how much your costs trend here, you may want to consider \" rightsizing \" to a different instance type that is allocated additional CPU. Contribute Help us improve this page by making a contribution on our Github repository .","title":"EC2-Other"},{"location":"aws/services/ec2-other-pricing/#summary","text":"EC2-Other is a category of AWS costs that typically causes the greatest amount of confusion for customers as it doesn't necessarily map to a single AWS service. EC2-Other encompasses the following costs: Usage Type Description EBS Volume Usage Usage for EBS Volumes . EBS Snapshot Usage Usage for EBS Snapshots . CPU Credits from t2/t3/t4g EC2 instances T-family EC2 Instances can carry potential CPU credit charges as described more below. NAT Gateway Usage Hourly usage for NAT Gateways. Data Transfer Idle Elastic IP Address usage AWS charges you for unattached IP addresses. It's typically good hygiene to occasionally monitor for stranded resources and clean them up.","title":"Summary"},{"location":"aws/services/ec2-other-pricing/#stranded-resources","text":"Unused or stranded EBS Volumes and IP Addresses can add up over time especially if these resources are created automatically as part of an autoscaling service where they're spun up but not down. You should consider occasionally auditing your unattached EBS Volumes and IP addresses to see if you can clean them up to save costs.","title":"Stranded Resources"},{"location":"aws/services/ec2-other-pricing/#what-are-t2t3t4g-cpu-credit-charges","text":"T2, T3 and T4g instances have a concept of \"Unlimited mode\" whereby you are charged a per-vCPU hour for bursting into this CPU usage. If you are leveraging these EC2 Instance Types with unlimited mode enabled, you should consider keeping an eye on these costs. Depending on how much your costs trend here, you may want to consider \" rightsizing \" to a different instance type that is allocated additional CPU. Contribute Help us improve this page by making a contribution on our Github repository .","title":"What are t2/t3/T4g CPU credit charges?"},{"location":"aws/services/ec2-pricing/","text":"Amazon EC2 Pricing Page Summary # Amazon EC2 (Elastic Cloud Compute) is Amazon\u2019s most popular service and usually one of the top cost centers for most companies. Amazon EC2 allows customers to create virtual private servers and has different pricing depending on the \u201cinstance type\u201d you use. Instance types are grouped into families with varying generations. Each instance type has a different mix of underlying hardware, allocated resources and as a result: pricing. Additionally, depending on the underlying software running on the EC2 instance you may be charged different rates. Pricing Dimensions # Dimension Description Instance Type Usage EC2 instance types are billed on one second increments, with a minimum of 60 seconds. For certain instance types with pre-installed software, you are billed in increments of hours. Instance Type Lifecycle EC2 has different lifecycle types - the two most often used are on-demand and spot. These concepts are discussed more in-depth below. AMI AMI stands for Amazon Machine Images. Depending on the AMI you use (i.e., Linux vs Windows) you potentially pay an additional amount of money on top of the instance type base usage. On-Demand vs Spot # By default, EC2 instances are launched in \"on-demand\" mode and charged accompanying on-demand rates which are the most expensive. AWS also offers \"Spot\" instances which can offer significant cost savings by using unused additional compute capacity. However, Spot tends to only work for fault-tolerant workloads as AWS can pre-empt and terminate these instances within two minutes if need be. Depending on your application's needs, you can consider using Spot instances for significant cost savings in the event you are comfortable with these instances being terminated. In general, your application's architecture should be comfortable with either 1) there being no spot instances available or 2) these instances being terminated. Autoscaling # EC2 autoscaling is provided by a primitive named Autoscaling Groups . Autoscaling Groups have lifecycle hooks to accommodate complex workflows regarding instance creation or termination and can support multiple instance types or spot instances using a Mixed Instance Policy . New instances are added based on a launch template (or launch config). This can be a challenge for organizations without good practices around creating machine images or automation for standing up applications. Rightsizing # Rightsizing refers to the process of ensuring that you're using the proper instance type suited for your application or workload. For example if you're using the largest instance type in a particular family but not using the CPU, Storage and Memory allocated to it fully you may be overpaying for what you need. Rightsizing is usually a manual process that involves engineering time for looking at a combination of application-level performance metrics like application CPU and Memory consumption and infrastructure-related attributes like what kind of underlying CPU powers an instance type. Savings Plans # EC2 Instances are covered by AWS Savings Plans. Savings Plans are covered more in depth as a general concept here . As it relates to EC2, Savings Plans are preferable as they present the same savings as Reserved Instances but aren't constrained to a single instance type. Reserved Instances # EC2 Instances are covered by AWS Reserved Instances. Reserved Instances are covered more in depth as a general concept here . As it relates to EC2, Reserved Instances aren't preferred as they present the same savings as Savings Plans but are constrained to a single instance type where as Savings Plans give greater flexibility. Generational Upgrades # EC2 instance types are grouped into families (discussed below) with multiple generations. For example a m4.4xlarge is of family type m and generation 4 . The next generation for the same instance type would be m5.4xlarge . Typically, as cloud infrastructure providers release new families it's cheaper and more performant to run the later generation instance types. Upgrade instances from one generation to another can be a major area of cost savings. Generation upgrades usually result in between 5% and 10% cost savings per generation and varies per family. Instance Type Families # EC2 Instance Types are organized into \"Families\" and each family can have multiple \"Generations\". By looking at each instance type you can infer its Family and Generation from the instance type name. For example, a c5.4xlarge is the c Family and 5th Generation. Below is a table of EC2 Instance Families and simple descriptions: Family Description a ARM Processors c Compute-optimized d Locally attached spinning HDD f Customizable hardware acceleration with FPGAs g Graphics GPU Instances h Large spinning HDD i NVMe SSD-backed storage optimized inf Machine-learning inference mac Apple Mac mini computers m General purpose with balanced CPU, memory and storage p General Purpose GPU r Memory-optimized t Burstable instances x Lowest price-per-GB RAM instances z Highest core frequency Contribute Help us improve this page by making a contribution on our Github repository .","title":"EC2"},{"location":"aws/services/ec2-pricing/#summary","text":"Amazon EC2 (Elastic Cloud Compute) is Amazon\u2019s most popular service and usually one of the top cost centers for most companies. Amazon EC2 allows customers to create virtual private servers and has different pricing depending on the \u201cinstance type\u201d you use. Instance types are grouped into families with varying generations. Each instance type has a different mix of underlying hardware, allocated resources and as a result: pricing. Additionally, depending on the underlying software running on the EC2 instance you may be charged different rates.","title":"Summary"},{"location":"aws/services/ec2-pricing/#pricing-dimensions","text":"Dimension Description Instance Type Usage EC2 instance types are billed on one second increments, with a minimum of 60 seconds. For certain instance types with pre-installed software, you are billed in increments of hours. Instance Type Lifecycle EC2 has different lifecycle types - the two most often used are on-demand and spot. These concepts are discussed more in-depth below. AMI AMI stands for Amazon Machine Images. Depending on the AMI you use (i.e., Linux vs Windows) you potentially pay an additional amount of money on top of the instance type base usage.","title":"Pricing Dimensions"},{"location":"aws/services/ec2-pricing/#on-demand-vs-spot","text":"By default, EC2 instances are launched in \"on-demand\" mode and charged accompanying on-demand rates which are the most expensive. AWS also offers \"Spot\" instances which can offer significant cost savings by using unused additional compute capacity. However, Spot tends to only work for fault-tolerant workloads as AWS can pre-empt and terminate these instances within two minutes if need be. Depending on your application's needs, you can consider using Spot instances for significant cost savings in the event you are comfortable with these instances being terminated. In general, your application's architecture should be comfortable with either 1) there being no spot instances available or 2) these instances being terminated.","title":"On-Demand vs Spot"},{"location":"aws/services/ec2-pricing/#autoscaling","text":"EC2 autoscaling is provided by a primitive named Autoscaling Groups . Autoscaling Groups have lifecycle hooks to accommodate complex workflows regarding instance creation or termination and can support multiple instance types or spot instances using a Mixed Instance Policy . New instances are added based on a launch template (or launch config). This can be a challenge for organizations without good practices around creating machine images or automation for standing up applications.","title":"Autoscaling"},{"location":"aws/services/ec2-pricing/#rightsizing","text":"Rightsizing refers to the process of ensuring that you're using the proper instance type suited for your application or workload. For example if you're using the largest instance type in a particular family but not using the CPU, Storage and Memory allocated to it fully you may be overpaying for what you need. Rightsizing is usually a manual process that involves engineering time for looking at a combination of application-level performance metrics like application CPU and Memory consumption and infrastructure-related attributes like what kind of underlying CPU powers an instance type.","title":"Rightsizing"},{"location":"aws/services/ec2-pricing/#savings-plans","text":"EC2 Instances are covered by AWS Savings Plans. Savings Plans are covered more in depth as a general concept here . As it relates to EC2, Savings Plans are preferable as they present the same savings as Reserved Instances but aren't constrained to a single instance type.","title":"Savings Plans"},{"location":"aws/services/ec2-pricing/#reserved-instances","text":"EC2 Instances are covered by AWS Reserved Instances. Reserved Instances are covered more in depth as a general concept here . As it relates to EC2, Reserved Instances aren't preferred as they present the same savings as Savings Plans but are constrained to a single instance type where as Savings Plans give greater flexibility.","title":"Reserved Instances"},{"location":"aws/services/ec2-pricing/#generational-upgrades","text":"EC2 instance types are grouped into families (discussed below) with multiple generations. For example a m4.4xlarge is of family type m and generation 4 . The next generation for the same instance type would be m5.4xlarge . Typically, as cloud infrastructure providers release new families it's cheaper and more performant to run the later generation instance types. Upgrade instances from one generation to another can be a major area of cost savings. Generation upgrades usually result in between 5% and 10% cost savings per generation and varies per family.","title":"Generational Upgrades"},{"location":"aws/services/ec2-pricing/#instance-type-families","text":"EC2 Instance Types are organized into \"Families\" and each family can have multiple \"Generations\". By looking at each instance type you can infer its Family and Generation from the instance type name. For example, a c5.4xlarge is the c Family and 5th Generation. Below is a table of EC2 Instance Families and simple descriptions: Family Description a ARM Processors c Compute-optimized d Locally attached spinning HDD f Customizable hardware acceleration with FPGAs g Graphics GPU Instances h Large spinning HDD i NVMe SSD-backed storage optimized inf Machine-learning inference mac Apple Mac mini computers m General purpose with balanced CPU, memory and storage p General Purpose GPU r Memory-optimized t Burstable instances x Lowest price-per-GB RAM instances z Highest core frequency Contribute Help us improve this page by making a contribution on our Github repository .","title":"Instance Type Families"},{"location":"aws/services/ecr-pricing/","text":"Amazon ECR Pricing Page Summary # Amazon Elastic Container Registry (ECR) is a fully managed container registry that allows you to store container images. You can create as many \"Repositories\" as you'd like that are free. As you push container \"Images\" to your repository, you're charged for the storage of these images which can acrue over time. Additionally, ECR charges different data transfer rates for private versus public repositories. Pricing Dimensions # Dimension Description Container Image Storage Amazon ECR charges a rate per month for the amount of storage per GB you store for container images. Data Transfer Amazon ECR charges different rates for data transfer from public and private repositories. Storage Costs per ECR Repository # Determining the cost per Container Repository can be a lot of effort, especially if you have a large quantity of images. To calculate the storage cost per container repository: List all of your container images Collect the image digest from each Determine just the unique digests of the layers in your container repository Get the size of each unique digest. If you prefer not to do this manually yourself, Vantage will compute the size and corresponding cost of all repositories automatically when you connect an AWS account. Lifecycle Policies # ECR stores every container image you push to a registry by default. Over time, the storage of all of these images can add up. Amazon offers a primitive called a \"Lifecycle Policy\" that allows you to set conditions for having Amazon clean up images on your behalf. There are two types of lifeycle policies: Lifecycle Policy Description imageCountMoreThan ECR allows you to define a certain number of images to retain and anything over that count will be cleaned up. For example if you set a Lifecycle Policy with a imageCountMoreThan value of 10, your most recent 10 images will always be kept. sinceImagePushed ECR allows you to set lifecycle policies with a value of sinceImagePushed which has a value of a certain number of days. So for example if you have a Lifecycle Policy applied with a sinceImagePushed value of 7, ECR will delete images as often as they are older than 7 days. Note : that when you apply a Lifecycle Policy, it is evaluated immediately. So if you have 500 images in a repositority and impose a lifeycle policy of 10 as soon as that policy is applied ECR will delete the 490 oldest images. Example imageCountMoreThan Lifeycle Policy # Here's an example of how to impose a Lifeycle Policy via the AWS CLI using the value of imageCountMoreThan: aws ecr put-lifecycle-policy \\ --repository-name \"vantage/mcyolo\" \\ --lifecycle-policy-text \"file://policy.json\" Where the content of the file for policy.json is the following: { \"rules\": [ { \"rulePriority\": 1, \"description\": \"Expire images over a count of 10\", \"selection\": { \"tagStatus\": \"untagged\", \"countType\": \"imageCountMoreThan\", \"countNumber\": 10 }, \"action\": { \"type\": \"expire\" } } ] } Example sinceImagePushed Lifeycle Policy # Here's an example of how to impose a Lifeycle Policy via the AWS CLI using the value of sinceImagePushed: aws ecr put-lifecycle-policy \\ --repository-name \"vantage/mcyolo\" \\ --lifecycle-policy-text \"file://policy.json\" Where the content of the file for policy.json is the following: { \"rules\": [ { \"rulePriority\": 1, \"description\": \"Expire images older than 14 days\", \"selection\": { \"tagStatus\": \"untagged\", \"countType\": \"sinceImagePushed\", \"countUnit\": \"days\", \"countNumber\": 14 }, \"action\": { \"type\": \"expire\" } } ] } Contribute Help us improve this page by making a contribution on our Github repository .","title":"ECR"},{"location":"aws/services/ecr-pricing/#summary","text":"Amazon Elastic Container Registry (ECR) is a fully managed container registry that allows you to store container images. You can create as many \"Repositories\" as you'd like that are free. As you push container \"Images\" to your repository, you're charged for the storage of these images which can acrue over time. Additionally, ECR charges different data transfer rates for private versus public repositories.","title":"Summary"},{"location":"aws/services/ecr-pricing/#pricing-dimensions","text":"Dimension Description Container Image Storage Amazon ECR charges a rate per month for the amount of storage per GB you store for container images. Data Transfer Amazon ECR charges different rates for data transfer from public and private repositories.","title":"Pricing Dimensions"},{"location":"aws/services/ecr-pricing/#storage-costs-per-ecr-repository","text":"Determining the cost per Container Repository can be a lot of effort, especially if you have a large quantity of images. To calculate the storage cost per container repository: List all of your container images Collect the image digest from each Determine just the unique digests of the layers in your container repository Get the size of each unique digest. If you prefer not to do this manually yourself, Vantage will compute the size and corresponding cost of all repositories automatically when you connect an AWS account.","title":"Storage Costs per ECR Repository"},{"location":"aws/services/ecr-pricing/#lifecycle-policies","text":"ECR stores every container image you push to a registry by default. Over time, the storage of all of these images can add up. Amazon offers a primitive called a \"Lifecycle Policy\" that allows you to set conditions for having Amazon clean up images on your behalf. There are two types of lifeycle policies: Lifecycle Policy Description imageCountMoreThan ECR allows you to define a certain number of images to retain and anything over that count will be cleaned up. For example if you set a Lifecycle Policy with a imageCountMoreThan value of 10, your most recent 10 images will always be kept. sinceImagePushed ECR allows you to set lifecycle policies with a value of sinceImagePushed which has a value of a certain number of days. So for example if you have a Lifecycle Policy applied with a sinceImagePushed value of 7, ECR will delete images as often as they are older than 7 days. Note : that when you apply a Lifecycle Policy, it is evaluated immediately. So if you have 500 images in a repositority and impose a lifeycle policy of 10 as soon as that policy is applied ECR will delete the 490 oldest images.","title":"Lifecycle Policies"},{"location":"aws/services/ecr-pricing/#example-imagecountmorethan-lifeycle-policy","text":"Here's an example of how to impose a Lifeycle Policy via the AWS CLI using the value of imageCountMoreThan: aws ecr put-lifecycle-policy \\ --repository-name \"vantage/mcyolo\" \\ --lifecycle-policy-text \"file://policy.json\" Where the content of the file for policy.json is the following: { \"rules\": [ { \"rulePriority\": 1, \"description\": \"Expire images over a count of 10\", \"selection\": { \"tagStatus\": \"untagged\", \"countType\": \"imageCountMoreThan\", \"countNumber\": 10 }, \"action\": { \"type\": \"expire\" } } ] }","title":"Example imageCountMoreThan Lifeycle Policy"},{"location":"aws/services/ecr-pricing/#example-sinceimagepushed-lifeycle-policy","text":"Here's an example of how to impose a Lifeycle Policy via the AWS CLI using the value of sinceImagePushed: aws ecr put-lifecycle-policy \\ --repository-name \"vantage/mcyolo\" \\ --lifecycle-policy-text \"file://policy.json\" Where the content of the file for policy.json is the following: { \"rules\": [ { \"rulePriority\": 1, \"description\": \"Expire images older than 14 days\", \"selection\": { \"tagStatus\": \"untagged\", \"countType\": \"sinceImagePushed\", \"countUnit\": \"days\", \"countNumber\": 14 }, \"action\": { \"type\": \"expire\" } } ] } Contribute Help us improve this page by making a contribution on our Github repository .","title":"Example sinceImagePushed Lifeycle Policy"},{"location":"aws/services/ecs-and-fargate-pricing/","text":"ECS Pricing Page Fargate Pricing Page Summary # Elastic Container Service (ECS) allows you to run docker containers through a primitive named a \"Task\". Tasks ultimately run on EC2 instances which are either managed by you (ECS on EC2) or fully managed by AWS (Fargate). There is no additional charge to you when using ECS on self-managed EC2 as you're just paying for EC2 instances that you create and manage. Fargate charges you for the vCPU and Memory for a ECS Task or EKS Pod and you pay a premium for managing the underlying EC2 instances. Fargate Pricing Dimensions # Dimension Description vCPU Hours When configuring a Fargate Task or EKS Pod you assign a certain amount vCPU and are charged a corresponding per-hour VCPU rate. GB Memory Hours When configuring a Fargate Task or EKS Pod you assign a certain amount GB of Memory and are charged a corresponding per-hour GB of Memory rate. Fargate Spot # Fargate has the ability to run in a Spot capacity which is conceptually the same premise as EC2 Spot - allowing you to run Tasks at up to a 70% discount off the Fargate on-demand price. When the capacity for Fargate Spot is available, you will be able to launch tasks based on your specified request. When AWS needs the capacity back, tasks running on Fargate Spot will be interrupted with two minutes of notification. If the capacity for Fargate Spot stops being available, Fargate will scale down tasks running on Fargate Spot while maintaining any regular tasks you are running. Fargate vs self-managed EC2 on ECS or EKS # Fargate charges a significant premium for managing the underlying nodes. Additionally, Fargate has varying degress of vCPU performance that differ depending on the Task. As a result, Fargate can have pitfalls relative to self-managed ECS or EKS on EC2 beyond just the additional costs. For a more in-depth article for seeing how Fargate is priced relative to self-managed EC2, please read the following blog post for understanding Fargate pricing . Contribute Help us improve this page by making a contribution on our Github repository .","title":"ECS & Fargate"},{"location":"aws/services/ecs-and-fargate-pricing/#summary","text":"Elastic Container Service (ECS) allows you to run docker containers through a primitive named a \"Task\". Tasks ultimately run on EC2 instances which are either managed by you (ECS on EC2) or fully managed by AWS (Fargate). There is no additional charge to you when using ECS on self-managed EC2 as you're just paying for EC2 instances that you create and manage. Fargate charges you for the vCPU and Memory for a ECS Task or EKS Pod and you pay a premium for managing the underlying EC2 instances.","title":"Summary"},{"location":"aws/services/ecs-and-fargate-pricing/#fargate-pricing-dimensions","text":"Dimension Description vCPU Hours When configuring a Fargate Task or EKS Pod you assign a certain amount vCPU and are charged a corresponding per-hour VCPU rate. GB Memory Hours When configuring a Fargate Task or EKS Pod you assign a certain amount GB of Memory and are charged a corresponding per-hour GB of Memory rate.","title":"Fargate Pricing Dimensions"},{"location":"aws/services/ecs-and-fargate-pricing/#fargate-spot","text":"Fargate has the ability to run in a Spot capacity which is conceptually the same premise as EC2 Spot - allowing you to run Tasks at up to a 70% discount off the Fargate on-demand price. When the capacity for Fargate Spot is available, you will be able to launch tasks based on your specified request. When AWS needs the capacity back, tasks running on Fargate Spot will be interrupted with two minutes of notification. If the capacity for Fargate Spot stops being available, Fargate will scale down tasks running on Fargate Spot while maintaining any regular tasks you are running.","title":"Fargate Spot"},{"location":"aws/services/ecs-and-fargate-pricing/#fargate-vs-self-managed-ec2-on-ecs-or-eks","text":"Fargate charges a significant premium for managing the underlying nodes. Additionally, Fargate has varying degress of vCPU performance that differ depending on the Task. As a result, Fargate can have pitfalls relative to self-managed ECS or EKS on EC2 beyond just the additional costs. For a more in-depth article for seeing how Fargate is priced relative to self-managed EC2, please read the following blog post for understanding Fargate pricing . Contribute Help us improve this page by making a contribution on our Github repository .","title":"Fargate vs self-managed EC2 on ECS or EKS"},{"location":"aws/services/elasticache-pricing/","text":"Amazon ElastiCache Pricing Page Summary # Amazon ElastiCache allows you to set up, run, and scale popular open-source compatible in-memory data stores like Redis or Memcached. ElastiCache ultimately runs atop EC2 instances with pre-configured software and are prefixed with \"cache.\" and are referred to as Nodes. Pricing Dimensions # Dimension Description Node Usage You are charged for the amount of hours your ElastiCache nodes run. Reserved Instances # ElastiCache Nodes do have Reserved Instances that can give you significant savings. Reserved Instances are covered as a general concept found here . Typically, as ElastiCache nodes remain on for longer durations and aren't members of auto-scaling groups, they are good candidates for cost savings via Reserved Instances. Savings Plans # ElastiCache Nodes are not covered under AWS Savings Plans. Contribute Help us improve this page by making a contribution on our Github repository .","title":"ElastiCache"},{"location":"aws/services/elasticache-pricing/#summary","text":"Amazon ElastiCache allows you to set up, run, and scale popular open-source compatible in-memory data stores like Redis or Memcached. ElastiCache ultimately runs atop EC2 instances with pre-configured software and are prefixed with \"cache.\" and are referred to as Nodes.","title":"Summary"},{"location":"aws/services/elasticache-pricing/#pricing-dimensions","text":"Dimension Description Node Usage You are charged for the amount of hours your ElastiCache nodes run.","title":"Pricing Dimensions"},{"location":"aws/services/elasticache-pricing/#reserved-instances","text":"ElastiCache Nodes do have Reserved Instances that can give you significant savings. Reserved Instances are covered as a general concept found here . Typically, as ElastiCache nodes remain on for longer durations and aren't members of auto-scaling groups, they are good candidates for cost savings via Reserved Instances.","title":"Reserved Instances"},{"location":"aws/services/elasticache-pricing/#savings-plans","text":"ElastiCache Nodes are not covered under AWS Savings Plans. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Savings Plans"},{"location":"aws/services/elasticsearch-pricing/","text":"Amazon Elasticsearch Service Pricing Page Summary # Amazon Elasticsearch Service is a full-managed service which runs ElasticSearch which is used primarily for querying JSOn based search and analytics data. Amazon ElasticSearch Service is billed per instance for the amount of EBS storage attached to the instance and the type of instance which is used to run the service. Pricing Dimensions # Dimension Description Instance Type Usage ElasticSearch instance types are billed at an hourly rate and charged that hourly rate on a per-second basis for your usage. Attached Storage ElasticSearch allows you to attach storage to the instances either as General Purpose Storage or Provisioned IOPS storage. Behind the scenes these are just managed EBS Volumes that ElasticSearch orchestrates on your behalf. However, on your monthly AWS bill these charges will show up under the ElasticSearch service and not under the EBS service. There are also options for high performance local SSD disks for storage optimized instances. Storage Optimized Instances # If better storage performance, above EBS, is needed you can select Storage Optmized instances which include local NVMe SSD disks. Reserved Instances # As ElasticSearch instances are not covered by AWS Savings Plans , you must rely on procuring Reserved Instances specifically for ElasticSearch. Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information. Contribute Help us improve this page by making a contribution on our Github repository .","title":"ElasticSearch"},{"location":"aws/services/elasticsearch-pricing/#summary","text":"Amazon Elasticsearch Service is a full-managed service which runs ElasticSearch which is used primarily for querying JSOn based search and analytics data. Amazon ElasticSearch Service is billed per instance for the amount of EBS storage attached to the instance and the type of instance which is used to run the service.","title":"Summary"},{"location":"aws/services/elasticsearch-pricing/#pricing-dimensions","text":"Dimension Description Instance Type Usage ElasticSearch instance types are billed at an hourly rate and charged that hourly rate on a per-second basis for your usage. Attached Storage ElasticSearch allows you to attach storage to the instances either as General Purpose Storage or Provisioned IOPS storage. Behind the scenes these are just managed EBS Volumes that ElasticSearch orchestrates on your behalf. However, on your monthly AWS bill these charges will show up under the ElasticSearch service and not under the EBS service. There are also options for high performance local SSD disks for storage optimized instances.","title":"Pricing Dimensions"},{"location":"aws/services/elasticsearch-pricing/#storage-optimized-instances","text":"If better storage performance, above EBS, is needed you can select Storage Optmized instances which include local NVMe SSD disks.","title":"Storage Optimized Instances"},{"location":"aws/services/elasticsearch-pricing/#reserved-instances","text":"As ElasticSearch instances are not covered by AWS Savings Plans , you must rely on procuring Reserved Instances specifically for ElasticSearch. Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Reserved Instances"},{"location":"aws/services/elb-pricing/","text":"Amazon ELB Pricing Page Summary # Amazon Elastic Load Balancer (ELB) is a service which distributes traffic from a single endpoint (public or private) to one or many private resources. Most commonly an Elastic Load Balancer will be exposed to the public internet and will distribute the incoming traffic to several app servers (usually running on EC2 or ECS). Elastic Load Balancers can also be used to distribute private traffic from one service to another. There are different options for the type of ELB and they are priced differently and come with different feature sets. Pricing Dimensions # Dimension Description Load Balancer Hours Every type of load balancer has a standard per hour rate and is always billed for a full hour. Load Balancer Data Processed Each type of load balancer has a formula for how the data processed by the load balancer is turned into an additional hourly charged. Application Load Balancer # Application Load Balancers (ALB) are useful for distributing layer 7 (HTTP, HTTPS, gRPC) traffic to application servers or other backends. ALBs have a standard hourly rate per region and a formula for calculating \"LCU\"-hours. The dimensions for calculating LCU are: Dimension Description New Connections A single LCU is 25 new connections per second. Active connections A single LCU is 3,000 active connections per minute. Processed bytes A single LCU is 1 GB per hour for EC2 instances, containers and IP addresses as targets and 0.4 GB per hour for Lambda functions as targets. Rule evaluations A single LCU is 1,000 rule evaluations per second. Whichever of these dimensions produces the highest LCU for an hour is what is used to create the charge for LCU-hour. Network Load Balancer # Network Load Balacers (NLB) are used for forwarding layer 4 traffic (TCP, UDP, TLS) to any other resoruce with an IP address. NLBs have a standard hourly rate per region and a formula for calculating \"NLCU\"-hours depending on the type of network traffic. The dimensions for calculating NCLU are: Dimension TCP UDP TLS New Connection or Flow 800 400 50 Active Connection or Flow 100,000 50,000 3,000 Processed bytes 1GB 1GB 1GB Gateway Load Balancer # Gateway Load Balancers are used to proxy traffic through third-party virtual appliances which support GENEVE. GLBs have a standard hourly rate per region and a formula for calculating \"GLCU\"-hours. The dimensions for calculating GLCU are: Dimension Description New Connections A single LCU is 600 new connections per second. Active connections A single LCU is 60,000 active connections per minute. Processed bytes A single LCU is 1 GB per hour for EC2 instances, containers and IP addresses as targets and 0.4 GB per hour for Lambda functions as targets. Classic Load Balancer # Classic load balancers are the original type of load balancer which has since been superceded by ALB and NLB. CLBs support both layer 7 and layer 4 traffic. CLBs have a standard hourly rate per region and a standard per GB rate per region for traffic processed. Contribute Help us improve this page by making a contribution on our Github repository .","title":"ELB"},{"location":"aws/services/elb-pricing/#summary","text":"Amazon Elastic Load Balancer (ELB) is a service which distributes traffic from a single endpoint (public or private) to one or many private resources. Most commonly an Elastic Load Balancer will be exposed to the public internet and will distribute the incoming traffic to several app servers (usually running on EC2 or ECS). Elastic Load Balancers can also be used to distribute private traffic from one service to another. There are different options for the type of ELB and they are priced differently and come with different feature sets.","title":"Summary"},{"location":"aws/services/elb-pricing/#pricing-dimensions","text":"Dimension Description Load Balancer Hours Every type of load balancer has a standard per hour rate and is always billed for a full hour. Load Balancer Data Processed Each type of load balancer has a formula for how the data processed by the load balancer is turned into an additional hourly charged.","title":"Pricing Dimensions"},{"location":"aws/services/elb-pricing/#application-load-balancer","text":"Application Load Balancers (ALB) are useful for distributing layer 7 (HTTP, HTTPS, gRPC) traffic to application servers or other backends. ALBs have a standard hourly rate per region and a formula for calculating \"LCU\"-hours. The dimensions for calculating LCU are: Dimension Description New Connections A single LCU is 25 new connections per second. Active connections A single LCU is 3,000 active connections per minute. Processed bytes A single LCU is 1 GB per hour for EC2 instances, containers and IP addresses as targets and 0.4 GB per hour for Lambda functions as targets. Rule evaluations A single LCU is 1,000 rule evaluations per second. Whichever of these dimensions produces the highest LCU for an hour is what is used to create the charge for LCU-hour.","title":"Application Load Balancer"},{"location":"aws/services/elb-pricing/#network-load-balancer","text":"Network Load Balacers (NLB) are used for forwarding layer 4 traffic (TCP, UDP, TLS) to any other resoruce with an IP address. NLBs have a standard hourly rate per region and a formula for calculating \"NLCU\"-hours depending on the type of network traffic. The dimensions for calculating NCLU are: Dimension TCP UDP TLS New Connection or Flow 800 400 50 Active Connection or Flow 100,000 50,000 3,000 Processed bytes 1GB 1GB 1GB","title":"Network Load Balancer"},{"location":"aws/services/elb-pricing/#gateway-load-balancer","text":"Gateway Load Balancers are used to proxy traffic through third-party virtual appliances which support GENEVE. GLBs have a standard hourly rate per region and a formula for calculating \"GLCU\"-hours. The dimensions for calculating GLCU are: Dimension Description New Connections A single LCU is 600 new connections per second. Active connections A single LCU is 60,000 active connections per minute. Processed bytes A single LCU is 1 GB per hour for EC2 instances, containers and IP addresses as targets and 0.4 GB per hour for Lambda functions as targets.","title":"Gateway Load Balancer"},{"location":"aws/services/elb-pricing/#classic-load-balancer","text":"Classic load balancers are the original type of load balancer which has since been superceded by ALB and NLB. CLBs support both layer 7 and layer 4 traffic. CLBs have a standard hourly rate per region and a standard per GB rate per region for traffic processed. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Classic Load Balancer"},{"location":"aws/services/lambda-pricing/","text":"Lambda Pricing Page Summary # AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. You are charged based on the number of requests for your functions and the duration, the time it takes for your code to execute. Pricing Dimensions # Dimension Description Number of requests You are charged $0.20 per 1M requests to your Lambda functions. Duration of request The price for duration depends on the amount of memory you allocate to your function. You can allocate any amount of memory to your function between 128MB and 10,240MB, in 1MB increments. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Lambda"},{"location":"aws/services/lambda-pricing/#summary","text":"AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. You are charged based on the number of requests for your functions and the duration, the time it takes for your code to execute.","title":"Summary"},{"location":"aws/services/lambda-pricing/#pricing-dimensions","text":"Dimension Description Number of requests You are charged $0.20 per 1M requests to your Lambda functions. Duration of request The price for duration depends on the amount of memory you allocate to your function. You can allocate any amount of memory to your function between 128MB and 10,240MB, in 1MB increments. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Pricing Dimensions"},{"location":"aws/services/rds-pricing/","text":"Amazon RDS Pricing Page Summary # Amazon Relational Database Service (RDS) provides you with the ability to create databases running certain software such as MySQL, Postgres, SQL Server and more. RDS instances ultimately are preconfigured EC2 instances running certain managed database software. As a result, you'll see similarities between instance types for RDS and EC2 where RDS instances are prefixed with \"db.\" Pricing Dimensions # Dimension Description Instance Type Usage RDS instance types are billed at an hourly rate and charged that hourly rate on a per-second basis for your usage. Database Software As RDS allows you to run different types of database software there are varying costs depending on which database software you choose to use. For example you can run Oracle and MySQL database on the same RDS instance types but they have different pricing as Oracle licensing contributes a higher cost than MySQL. Availability RDS allows you to run RDS instances in either \"Single AZ\" or \"Multi AZ\" deployments. \"Multi AZ\" deployments are more highly available but carry a larger cost. Attached Storage RDS allows you to attach storage to RDS instances either as General Purpose Storage or Provisioned IOPS storage. Behind the scenes these are just managed EBS Volumes that RDS orchestrates on your behalf. However, on your monthly AWS bill these charges will show up under the RDS service and not under the EBS service. Backup Storage You have the ability to turn on backups for your RDS instances and are charged an accompanying storage rate for backups. Reserved Instances # As RDS instances are not covered by AWS Savings Plans , you must rely on procuring Reserved Instances specifically for RDS. Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information. Single vs Multi Availability Zones # RDS allows you to deploy instances in either a single availability zone or across multiple availability zones. Shorthand, this is referenced as either \"single-AZ\" or \"multi-AZ\". The benefit of being multi-AZ is that you're provided with enhanced availability and durability for your database as Amazon provisions and maintains a standby in a different availability zone for automatic failover in the event of a scheduled or unplanned outage. From a cost consideration perspective, multi-AZ rates are double what single-AZ rates are for the added durability that you're provided. Contribute Help us improve this page by making a contribution on our Github repository .","title":"RDS"},{"location":"aws/services/rds-pricing/#summary","text":"Amazon Relational Database Service (RDS) provides you with the ability to create databases running certain software such as MySQL, Postgres, SQL Server and more. RDS instances ultimately are preconfigured EC2 instances running certain managed database software. As a result, you'll see similarities between instance types for RDS and EC2 where RDS instances are prefixed with \"db.\"","title":"Summary"},{"location":"aws/services/rds-pricing/#pricing-dimensions","text":"Dimension Description Instance Type Usage RDS instance types are billed at an hourly rate and charged that hourly rate on a per-second basis for your usage. Database Software As RDS allows you to run different types of database software there are varying costs depending on which database software you choose to use. For example you can run Oracle and MySQL database on the same RDS instance types but they have different pricing as Oracle licensing contributes a higher cost than MySQL. Availability RDS allows you to run RDS instances in either \"Single AZ\" or \"Multi AZ\" deployments. \"Multi AZ\" deployments are more highly available but carry a larger cost. Attached Storage RDS allows you to attach storage to RDS instances either as General Purpose Storage or Provisioned IOPS storage. Behind the scenes these are just managed EBS Volumes that RDS orchestrates on your behalf. However, on your monthly AWS bill these charges will show up under the RDS service and not under the EBS service. Backup Storage You have the ability to turn on backups for your RDS instances and are charged an accompanying storage rate for backups.","title":"Pricing Dimensions"},{"location":"aws/services/rds-pricing/#reserved-instances","text":"As RDS instances are not covered by AWS Savings Plans , you must rely on procuring Reserved Instances specifically for RDS. Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information.","title":"Reserved Instances"},{"location":"aws/services/rds-pricing/#single-vs-multi-availability-zones","text":"RDS allows you to deploy instances in either a single availability zone or across multiple availability zones. Shorthand, this is referenced as either \"single-AZ\" or \"multi-AZ\". The benefit of being multi-AZ is that you're provided with enhanced availability and durability for your database as Amazon provisions and maintains a standby in a different availability zone for automatic failover in the event of a scheduled or unplanned outage. From a cost consideration perspective, multi-AZ rates are double what single-AZ rates are for the added durability that you're provided. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Single vs Multi Availability Zones"},{"location":"aws/services/route-53-pricing/","text":"Route53 Pricing Page Summary # Summary # Amazon Route 53 is a Domain Name System (DNS) web service. Typically Route 53 doesn't tend to be a large cost center for the vast majority of companies. Pricing Dimensions # Dimension Description Hosted Zones You are charged either $0.50 or $0.10 per hosted zone per month depending on the number of hosted zones you have. You are charged $0.50 per hosted zone per month for the first 25 hosted zones and $0.10 per hosted zone per month for additional hosted zones. DNS Queries You incur charges for every DNS query answered by the Amazon Route 53 service, except for queries to Alias A records that are mapped to Elastic Load Balancing instances, CloudFront distributions, AWS Elastic Beanstalk environments, API Gateways, VPC endpoints, or Amazon S3 website buckets, which are provided at no additional charge. Registered Domain Names You pay an annual charge for each domain name registered via or transferred into Route 53. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Route 53"},{"location":"aws/services/route-53-pricing/#summary","text":"","title":"Summary"},{"location":"aws/services/route-53-pricing/#summary_1","text":"Amazon Route 53 is a Domain Name System (DNS) web service. Typically Route 53 doesn't tend to be a large cost center for the vast majority of companies.","title":"Summary"},{"location":"aws/services/route-53-pricing/#pricing-dimensions","text":"Dimension Description Hosted Zones You are charged either $0.50 or $0.10 per hosted zone per month depending on the number of hosted zones you have. You are charged $0.50 per hosted zone per month for the first 25 hosted zones and $0.10 per hosted zone per month for additional hosted zones. DNS Queries You incur charges for every DNS query answered by the Amazon Route 53 service, except for queries to Alias A records that are mapped to Elastic Load Balancing instances, CloudFront distributions, AWS Elastic Beanstalk environments, API Gateways, VPC endpoints, or Amazon S3 website buckets, which are provided at no additional charge. Registered Domain Names You pay an annual charge for each domain name registered via or transferred into Route 53. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Pricing Dimensions"},{"location":"aws/services/s3-pricing/","text":"Amazon S3 Pricing Page Summary # Amazon Simple Storage Service (S3) is an object storage service that allows customers to storage files which are known as \"objects\". Objects are organized into namespaces named \"buckets\" for which there is no additional cost for having. Ultimately you are charged on the dimensions below but are a mix of how much you store with specific storage types, the bandwidth for accessing those files and the requests you make to the S3 service. Pricing Dimensions # Dimension Description Object Storage Amount Amazon S3 charges you for how much you store across all objects across all buckets. There is different pricing rates per region on a per-GB basis and as you store more data on S3, you get discounts on a per-GB basis. Object Storage Class Amazon S3 has many different Storage Classes which are discussed below. \"Standard Storage\" is the default storage class but you can get discounts for other tiers. Bandwidth Amazon S3 charges you for the amount of egress you consume for accessing S3 objects. You should keep an eye on how much bandwidth is being consumed especially if files are left open to the public where you can potentially have runaway costs if signifcant usage occurs. Request Metrics Amazon charges you for GET, SELECT, PUT, COPY, POST and LIST requests. Amazon also charges you different rates depending on which of these request types you're using. This is oftentimes an unknown cost that customers occur that you should keep an eye on. Intelligent Tiering # S3 Intelligent Tiering is an Amazon S3 storage class that automatically will optimize storage costs automatically on behalf of customers. S3 Intelligent Tiering will monitor access patterns of S3 objects on your behalf and shift them between four different storage classes on your behalf to deliver you with savings automatically. Typically customers have files that they store with the storage class of Standard Storage but don't think to ever optimize these costs and overpay for the the amount they're storing in S3. By using Intelligent Tiering, customers can focus on their application development and allow S3 Intelligent Tiering to manage shifting their objects' storage classes on their behalf. Understanding Storage Classes # S3 currently supports 19 different object storage types within an S3 Bucket. Each bucket is capable of holding objects from a single class or multiple classes. A light overview of these storage types are below: Storage Type Description Standard Storage\u200d Standard Storage (StandardStorage) is for general purpose storage for any type of data, typically used for frequently accessed data. Standard Storage is priced on a tiered basis where it gets incrementally cheaper to store data as you store more. Intelligent Tiering - Frequent Access (IntelligentTieringFAStorage) Objects uploaded to S3 Intelligent Tiering are automatically stored in the frequent access tier which has the same rates as Standard Storage. Intelligent Tiering Infrequent Access (IntelligentTieringIAStorage): Objects in Frequent Access that haven't been accessed in 30 consecutive days are moved to this tier in which prices drop significantly. Intelligent Tiering - Archive Access (IntelligentTieringAAStorage) Upon activating the archive access tier for intelligent tiering, S3 will automatically move objects that haven't been accessed for 90 days to archive access where the pricing is the same as Glacier. Intelligent Tiering - Deep Archive Access (IntelligentTieringDAAStorage) Upon activating the deep archive access tier for intelligent tiering, S3 will automatically move objects that haven't been accessed for 180 days to deep archive access. S3 Standard - Infrequent Access (StandardIAStorage) S3 Standard Infrequent Access is for data that is accessed less frequently, but requires rapid access when needed. It offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. Standard Infrequenty Access Overhead (StandardIASizeOverhead) There is a minimum billable size of 128KB. For example if you stored an object at 28KB, the StandardIASizeOverhead rate would increase by 128KB-28KB or 100KB and represented by this metric. S3 Standard - Infrequent Access (One Zone) S3 Infrequent Access One Zone is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones, S3 Infrequent Access One Zone stores data in a single AZ and costs 20% less than S3 Standard Infrequent Access. One Zone Size Overhead (OneZoneIASizeOverhead) There is a minimum billable size of 128KB. For example if you stored an object at 28KB, the StandardIASizeOverhead rate would increase by 128KB-28KB or 100KB and represented by this metric. S3 Glacier (GlacierStorage) S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours. S3 Glacier Overhead (GlacierObjectOverhead) For each object that is stored in S3 Glacier, 40 KB of chargeable overhead is added for metadata S3 Glacier Object Overhead (GlacierObjectOverhead) Amazon S3 Glacier also requires an additional 32KB of data per object for S3 Glacier\u2019s index and metadata. S3 Glacier Deep Archive (DeepArchiveStorage) S3 Glacier Deep Archive is Amazon S3\u2019s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers \u2014 particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors \u2014 that retain data sets for 7-10 years or longer to meet regulatory compliance requirements. S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. Deep Archive Object Overhead (DeepArchiveObjectOverhead) For each object that is stored in S3 Glacier, 40 KB of chargeable overhead is added for metadata. Deep Archive S3 Object Overhead (DeepArchiveS3ObjectOverhead) Amazon S3 Deep Archive also requires an additional 32KB of data per object for S3 Deep Archive index and metadata. Deep Archive Staging Storage (DeepArchiveStagingStorage) Staging storage is where the parts of Multipart Upload are staged until the CompleteMultipart request is issued. The parts are staged in S3 standard, and storage is charged at the S3 Standard price. S3 Reduced Redundancy Storage Reduced Redundancy Storage is an Amazon S3 storage option that enables customers to store noncritical, reproducible data at lower levels of redundancy than Amazon S3\u2019s standard storage. It provides a highly available solution for distributing or sharing content that is durably stored elsewhere, or for storing thumbnails, transcoded media, or other processed data that can be easily reproduced. The Reduced Redundancy option stores objects on multiple devices across multiple facilities, providing 400 times the durability of a typical disk drive, but does not replicate objects as many times as standard Amazon S3 storage. S3 Bucket Request Metrics # S3 does not have ingress, egress or request metrics turned on by default, leaving many users unsure of what their costs will be until they receive their Monthly AWS Bill . That being said, it's relatively easy to enable these metrics. Below is an example of how to enable these metrics for a S3 Bucket via the AWS CLI. Just be sure to replace YOUR_BUCKET_NAME with your actual bucket name and YOUR_BUCKET_REGION with the appropriate bucket region. aws s3api put-bucket-metrics-configuration --bucket YOUR_BUCKET_NAME --metrics-configuration Id=EntireBucket --id EntireBucket --region YOUR_BUCKET_REGION Note : it takes roughly 15 minutes for AWS to begin delivering these metrics after being enabled. S3 Versus Cloudflare Bandwidth Alliance Partner # The Cloudflare Bandwidth Alliance is a group of infrastructure providers that have decided to either completely waive or massively discount egress fees for shared customers. This can be a huge source of savings for customers that have an AWS bill where S3 egress costs make up a large portion of the aforementioned bill. By utilizing Cloudflare's content delivery network (CDN) service in tandem with a Bandwidth Alliance provider, customer can get no-cost content transit from their origin server to Cloudflare servers distributed around the world. This effectively reproduces the cost benefit that users get for pairing CloudFront with an AWS service like S3. 1 Utilizing one of Cloudflare's self-serve plans, customer can also cap their cost to deliver content via flat-rate pricing. Further details can be found at in the CloudFront service article of the Cloud Cost Handbook. Considerations # Price is not the only consideration that goes into making a decision about whether to utilize S3 or a competing storage service. Performance, availability, user experience, support and legal compliance are other factors that will factor into the decision to utilize one service over another. Complexity # AWS had made it exceedingly easy for customer to utilize other AWS service in tandem. There is a non-trivial cost for an organization to decide to split their infrastructure over multiple service providers. Developers will need to learn and understand both systems and when to choose one design pattern over the other. There will be two sets of documentation that will need to be addressed when designing or troubleshooting systems. Use-cases # The primary use-case in favor of utilizing this cost efficiency architecture strategy is if a user has a large amount of static content that is stored on S3 and being served to end-users via the internet. Contribute Help us improve this page by making a contribution on our Github repository . \"If you are using an AWS origin, effective December 1, 2014, data transferred from origin to edge locations (Amazon CloudFront \"origin fetches\") will be free of charge.\" https://aws.amazon.com/cloudfront/pricing/ \u21a9","title":"S3"},{"location":"aws/services/s3-pricing/#summary","text":"Amazon Simple Storage Service (S3) is an object storage service that allows customers to storage files which are known as \"objects\". Objects are organized into namespaces named \"buckets\" for which there is no additional cost for having. Ultimately you are charged on the dimensions below but are a mix of how much you store with specific storage types, the bandwidth for accessing those files and the requests you make to the S3 service.","title":"Summary"},{"location":"aws/services/s3-pricing/#pricing-dimensions","text":"Dimension Description Object Storage Amount Amazon S3 charges you for how much you store across all objects across all buckets. There is different pricing rates per region on a per-GB basis and as you store more data on S3, you get discounts on a per-GB basis. Object Storage Class Amazon S3 has many different Storage Classes which are discussed below. \"Standard Storage\" is the default storage class but you can get discounts for other tiers. Bandwidth Amazon S3 charges you for the amount of egress you consume for accessing S3 objects. You should keep an eye on how much bandwidth is being consumed especially if files are left open to the public where you can potentially have runaway costs if signifcant usage occurs. Request Metrics Amazon charges you for GET, SELECT, PUT, COPY, POST and LIST requests. Amazon also charges you different rates depending on which of these request types you're using. This is oftentimes an unknown cost that customers occur that you should keep an eye on.","title":"Pricing Dimensions"},{"location":"aws/services/s3-pricing/#intelligent-tiering","text":"S3 Intelligent Tiering is an Amazon S3 storage class that automatically will optimize storage costs automatically on behalf of customers. S3 Intelligent Tiering will monitor access patterns of S3 objects on your behalf and shift them between four different storage classes on your behalf to deliver you with savings automatically. Typically customers have files that they store with the storage class of Standard Storage but don't think to ever optimize these costs and overpay for the the amount they're storing in S3. By using Intelligent Tiering, customers can focus on their application development and allow S3 Intelligent Tiering to manage shifting their objects' storage classes on their behalf.","title":"Intelligent Tiering"},{"location":"aws/services/s3-pricing/#understanding-storage-classes","text":"S3 currently supports 19 different object storage types within an S3 Bucket. Each bucket is capable of holding objects from a single class or multiple classes. A light overview of these storage types are below: Storage Type Description Standard Storage\u200d Standard Storage (StandardStorage) is for general purpose storage for any type of data, typically used for frequently accessed data. Standard Storage is priced on a tiered basis where it gets incrementally cheaper to store data as you store more. Intelligent Tiering - Frequent Access (IntelligentTieringFAStorage) Objects uploaded to S3 Intelligent Tiering are automatically stored in the frequent access tier which has the same rates as Standard Storage. Intelligent Tiering Infrequent Access (IntelligentTieringIAStorage): Objects in Frequent Access that haven't been accessed in 30 consecutive days are moved to this tier in which prices drop significantly. Intelligent Tiering - Archive Access (IntelligentTieringAAStorage) Upon activating the archive access tier for intelligent tiering, S3 will automatically move objects that haven't been accessed for 90 days to archive access where the pricing is the same as Glacier. Intelligent Tiering - Deep Archive Access (IntelligentTieringDAAStorage) Upon activating the deep archive access tier for intelligent tiering, S3 will automatically move objects that haven't been accessed for 180 days to deep archive access. S3 Standard - Infrequent Access (StandardIAStorage) S3 Standard Infrequent Access is for data that is accessed less frequently, but requires rapid access when needed. It offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. Standard Infrequenty Access Overhead (StandardIASizeOverhead) There is a minimum billable size of 128KB. For example if you stored an object at 28KB, the StandardIASizeOverhead rate would increase by 128KB-28KB or 100KB and represented by this metric. S3 Standard - Infrequent Access (One Zone) S3 Infrequent Access One Zone is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones, S3 Infrequent Access One Zone stores data in a single AZ and costs 20% less than S3 Standard Infrequent Access. One Zone Size Overhead (OneZoneIASizeOverhead) There is a minimum billable size of 128KB. For example if you stored an object at 28KB, the StandardIASizeOverhead rate would increase by 128KB-28KB or 100KB and represented by this metric. S3 Glacier (GlacierStorage) S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours. S3 Glacier Overhead (GlacierObjectOverhead) For each object that is stored in S3 Glacier, 40 KB of chargeable overhead is added for metadata S3 Glacier Object Overhead (GlacierObjectOverhead) Amazon S3 Glacier also requires an additional 32KB of data per object for S3 Glacier\u2019s index and metadata. S3 Glacier Deep Archive (DeepArchiveStorage) S3 Glacier Deep Archive is Amazon S3\u2019s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers \u2014 particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors \u2014 that retain data sets for 7-10 years or longer to meet regulatory compliance requirements. S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. Deep Archive Object Overhead (DeepArchiveObjectOverhead) For each object that is stored in S3 Glacier, 40 KB of chargeable overhead is added for metadata. Deep Archive S3 Object Overhead (DeepArchiveS3ObjectOverhead) Amazon S3 Deep Archive also requires an additional 32KB of data per object for S3 Deep Archive index and metadata. Deep Archive Staging Storage (DeepArchiveStagingStorage) Staging storage is where the parts of Multipart Upload are staged until the CompleteMultipart request is issued. The parts are staged in S3 standard, and storage is charged at the S3 Standard price. S3 Reduced Redundancy Storage Reduced Redundancy Storage is an Amazon S3 storage option that enables customers to store noncritical, reproducible data at lower levels of redundancy than Amazon S3\u2019s standard storage. It provides a highly available solution for distributing or sharing content that is durably stored elsewhere, or for storing thumbnails, transcoded media, or other processed data that can be easily reproduced. The Reduced Redundancy option stores objects on multiple devices across multiple facilities, providing 400 times the durability of a typical disk drive, but does not replicate objects as many times as standard Amazon S3 storage.","title":"Understanding Storage Classes"},{"location":"aws/services/s3-pricing/#s3-bucket-request-metrics","text":"S3 does not have ingress, egress or request metrics turned on by default, leaving many users unsure of what their costs will be until they receive their Monthly AWS Bill . That being said, it's relatively easy to enable these metrics. Below is an example of how to enable these metrics for a S3 Bucket via the AWS CLI. Just be sure to replace YOUR_BUCKET_NAME with your actual bucket name and YOUR_BUCKET_REGION with the appropriate bucket region. aws s3api put-bucket-metrics-configuration --bucket YOUR_BUCKET_NAME --metrics-configuration Id=EntireBucket --id EntireBucket --region YOUR_BUCKET_REGION Note : it takes roughly 15 minutes for AWS to begin delivering these metrics after being enabled.","title":"S3 Bucket Request Metrics"},{"location":"aws/services/s3-pricing/#s3-versus-cloudflare-bandwidth-alliance-partner","text":"The Cloudflare Bandwidth Alliance is a group of infrastructure providers that have decided to either completely waive or massively discount egress fees for shared customers. This can be a huge source of savings for customers that have an AWS bill where S3 egress costs make up a large portion of the aforementioned bill. By utilizing Cloudflare's content delivery network (CDN) service in tandem with a Bandwidth Alliance provider, customer can get no-cost content transit from their origin server to Cloudflare servers distributed around the world. This effectively reproduces the cost benefit that users get for pairing CloudFront with an AWS service like S3. 1 Utilizing one of Cloudflare's self-serve plans, customer can also cap their cost to deliver content via flat-rate pricing. Further details can be found at in the CloudFront service article of the Cloud Cost Handbook.","title":"S3 Versus Cloudflare Bandwidth Alliance Partner"},{"location":"aws/services/s3-pricing/#considerations","text":"Price is not the only consideration that goes into making a decision about whether to utilize S3 or a competing storage service. Performance, availability, user experience, support and legal compliance are other factors that will factor into the decision to utilize one service over another.","title":"Considerations"},{"location":"aws/services/s3-pricing/#complexity","text":"AWS had made it exceedingly easy for customer to utilize other AWS service in tandem. There is a non-trivial cost for an organization to decide to split their infrastructure over multiple service providers. Developers will need to learn and understand both systems and when to choose one design pattern over the other. There will be two sets of documentation that will need to be addressed when designing or troubleshooting systems.","title":"Complexity"},{"location":"aws/services/s3-pricing/#use-cases","text":"The primary use-case in favor of utilizing this cost efficiency architecture strategy is if a user has a large amount of static content that is stored on S3 and being served to end-users via the internet. Contribute Help us improve this page by making a contribution on our Github repository . \"If you are using an AWS origin, effective December 1, 2014, data transferred from origin to edge locations (Amazon CloudFront \"origin fetches\") will be free of charge.\" https://aws.amazon.com/cloudfront/pricing/ \u21a9","title":"Use-cases"},{"location":"aws/services/vpc-pricing/","text":"Amazon VPC Pricing Page Summary # Amazon Virtual Private Cloud (VPC) is a service which allows customers to logically isolate their resources into different networks. Unless explicitly configured every VPC is completely isolated from every other VPC. There is no charge for a VPC in itself, however some optional sub-components of a VPC can incur charges. Pricing Dimensions # Dimension Description NAT Gateway Usage NAT Gateways are billed per hour at a standard rate per region. Each partial hour is billed as a full hour. NAT Gateway Transfer NAT Gateways are billed per GB which is processed by the gateway regaurdless of where the data is being transferred to or from. NAT Gateway # NAT (Network Address Translation) Gateways enable resources running inside of VPCs to connect to services outside of the VPC without needing to have those resources exposed to the public internet. Besides the standard usage and transfer charges on NAT Gateways you will also be charged standard bandwidth transfer charges on top of that depending on where the traffic is going. Amazon VPC Endpoints # VPC Endpoints allow resources to connect to other AWS services outside of a VPC, such as S3, without the need for a NAT Gateway. This is a good way to prevent NAT Gateway usage and transfer charges. Contribute Help us improve this page by making a contribution on our Github repository .","title":"VPC"},{"location":"aws/services/vpc-pricing/#summary","text":"Amazon Virtual Private Cloud (VPC) is a service which allows customers to logically isolate their resources into different networks. Unless explicitly configured every VPC is completely isolated from every other VPC. There is no charge for a VPC in itself, however some optional sub-components of a VPC can incur charges.","title":"Summary"},{"location":"aws/services/vpc-pricing/#pricing-dimensions","text":"Dimension Description NAT Gateway Usage NAT Gateways are billed per hour at a standard rate per region. Each partial hour is billed as a full hour. NAT Gateway Transfer NAT Gateways are billed per GB which is processed by the gateway regaurdless of where the data is being transferred to or from.","title":"Pricing Dimensions"},{"location":"aws/services/vpc-pricing/#nat-gateway","text":"NAT (Network Address Translation) Gateways enable resources running inside of VPCs to connect to services outside of the VPC without needing to have those resources exposed to the public internet. Besides the standard usage and transfer charges on NAT Gateways you will also be charged standard bandwidth transfer charges on top of that depending on where the traffic is going.","title":"NAT Gateway"},{"location":"aws/services/vpc-pricing/#amazon-vpc-endpoints","text":"VPC Endpoints allow resources to connect to other AWS services outside of a VPC, such as S3, without the need for a NAT Gateway. This is a good way to prevent NAT Gateway usage and transfer charges. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Amazon VPC Endpoints"},{"location":"aws/services/workspaces-pricing/","text":"Amazon WorkSpaces Pricing Page Summary # Amazon WorkSpaces is a fully managed, persistent desktop virtualization service. You can use Amazon WorkSpaces to provision either Windows or Linux desktops - each come with their own set of pricing implications discussed below. WorkSpace pricing can either be done in a monthly or hourly fashion. Pricing Dimensions # Dimension Description Compute Type WorkSpaces offers seven different types of compute types. They are Value , Standard , Performance , Power and PowerPro , Graphics , GraphicsPro . Each of these classes has a different set of underlying resources that contribute to costs differently. The order that these classes are listed in are from cheapest to most expensive. Platform Type Linux or Windows. You are charged an additional amount of money for running on Windows vs Linux. You may also bring your own license for Windows WorkSpaces to reduce Windows licensing costs if you have that available. Running Mode AUTO_STOP or ALWAYS_ON . When you choose AUTO_STOP you are choosing to create a WorkSpace that has a pre-deteremined expiration time in which that WorkSpace will terminate and is billed per hour. When you choose ALWAYS_ON you are charged on a monthly rate basis and the WorkSpace will persist being on until you take action to terminate it. WorkSpace Size Each Compute Type offers four different configurations with different amounts of vCPU and GB of Memory. Graphic and GraphicsPro only offer one size. Depending on the size you choose, you will pay a more expensive rate. Monitoring Unused WorkSpaces # WorkSpaces have an attribute named last_known_user_connection_timestamp that maintains a timestamp of when was the last time a user has accessed this WorkSpace. You should periodically audit WorkSpaces to ensure that they're being used as otherwise it can be wasteful and a contributor to costs. In the event that this timestamp isn't present, it means that a user has never actually connected to this instance. Additionally, you can look for a certain amount of time that has progressed since a user has accessed it - in the event that a user hasn't accessed a WorkSpace in over a few weeks, it may be a good candidate for clean up and cost savings. Contribute Help us improve this page by making a contribution on our Github repository .","title":"WorkSpaces"},{"location":"aws/services/workspaces-pricing/#summary","text":"Amazon WorkSpaces is a fully managed, persistent desktop virtualization service. You can use Amazon WorkSpaces to provision either Windows or Linux desktops - each come with their own set of pricing implications discussed below. WorkSpace pricing can either be done in a monthly or hourly fashion.","title":"Summary"},{"location":"aws/services/workspaces-pricing/#pricing-dimensions","text":"Dimension Description Compute Type WorkSpaces offers seven different types of compute types. They are Value , Standard , Performance , Power and PowerPro , Graphics , GraphicsPro . Each of these classes has a different set of underlying resources that contribute to costs differently. The order that these classes are listed in are from cheapest to most expensive. Platform Type Linux or Windows. You are charged an additional amount of money for running on Windows vs Linux. You may also bring your own license for Windows WorkSpaces to reduce Windows licensing costs if you have that available. Running Mode AUTO_STOP or ALWAYS_ON . When you choose AUTO_STOP you are choosing to create a WorkSpace that has a pre-deteremined expiration time in which that WorkSpace will terminate and is billed per hour. When you choose ALWAYS_ON you are charged on a monthly rate basis and the WorkSpace will persist being on until you take action to terminate it. WorkSpace Size Each Compute Type offers four different configurations with different amounts of vCPU and GB of Memory. Graphic and GraphicsPro only offer one size. Depending on the size you choose, you will pay a more expensive rate.","title":"Pricing Dimensions"},{"location":"aws/services/workspaces-pricing/#monitoring-unused-workspaces","text":"WorkSpaces have an attribute named last_known_user_connection_timestamp that maintains a timestamp of when was the last time a user has accessed this WorkSpace. You should periodically audit WorkSpaces to ensure that they're being used as otherwise it can be wasteful and a contributor to costs. In the event that this timestamp isn't present, it means that a user has never actually connected to this instance. Additionally, you can look for a certain amount of time that has progressed since a user has accessed it - in the event that a user hasn't accessed a WorkSpace in over a few weeks, it may be a good candidate for clean up and cost savings. Contribute Help us improve this page by making a contribution on our Github repository .","title":"Monitoring Unused WorkSpaces"}]}